{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CAMERA-READY PLOTTING (thanks Alex Boyd!)\n",
    "# -----------------------------------------------------------------------\n",
    "# Following code is borrowed from material provided by Alex!\n",
    "FULL_WIDTH = 5.50107\n",
    "COL_WIDTH  = 4.50461\n",
    "\n",
    "\n",
    "# Put at top of plotting script (requires tex be installed though)\n",
    "matplotlib.rc('font', family='serif', size=20)\n",
    "matplotlib.rc('text', usetex=True)\n",
    "\n",
    "\n",
    "def adjust(fig, left=0.0, right=1.0, bottom=0.0, top=1.0, wspace=0.0, hspace=0.0):\n",
    "    fig.subplots_adjust(\n",
    "        left   = left,  # the left side of the subplots of the figure\n",
    "        right  = right,  # the right side of the subplots of the figure\n",
    "        bottom = bottom,  # the bottom of the subplots of the figure\n",
    "        top    = top,  # the top of the subplots of the figure\n",
    "        wspace = wspace,  # the amount of width reserved for blank space between subplots\n",
    "        hspace = hspace,  # the amount of height reserved for white space between subplots\n",
    "    )\n",
    "    \n",
    "\n",
    "def save_fig(fig, name, **kwargs):\n",
    "    fig.savefig(f\"./camera_ready/images/{name}.pdf\", bbox_inches=\"tight\", **kwargs)\n",
    "\n",
    "\n",
    "# Axes formatting\n",
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "\n",
    "g\n",
    "# Accessibility\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "matplotlib.rcParams[\"axes.prop_cycle\"] = matplotlib.cycler(color=sns.color_palette(\"colorblind\"))\n",
    "\n",
    "\n",
    "# Composite plots \n",
    "def disable_axis(ax):\n",
    "    ax.set_zorder(-100)  # Avoids a visual rendering bug\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.setp(ax.spines.values(), color=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db7679",
   "metadata": {},
   "source": [
    "## Evaluating sentence-level differences between the different benchmarks\n",
    "\n",
    "\n",
    "In this notebook, we will focus on trying to characterize the differences between the different benchmarks.\n",
    "Before we delve into the analysis, let us define the properties we're interested in this work:\n",
    "1. **words** that may (or not) be associated with gender, deemed *gender co-occurring words*; \n",
    "2. whether the **sentences are *neutral* or not**, irrespective of whether the words are associated with gender or not. We dub these sentences - *gender-invariant sentences*.\n",
    "\n",
    "\n",
    "For 2., we have curated a set of prompts and conducted human evaluation to validate our claims. That is, we've characterized the distribution and showed that both pronouns fit. This validation verifies for our datasets as well as the coreference datasets, since they are created to be neutral by default.\n",
    "\n",
    "\n",
    "We conjecture that by means of our prompts, the generated sentences are gender-invariant. Hence, this notebook will carry the evaluation for **1.**, i.e., as we vary the strength of association between gender and occurring words.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The remaining of the notebook is organized as follows: \n",
    "\n",
    "1. **Data loading**: we load both PMI difference values and the scores given by the models to each test sentence pair.\n",
    "2. **Data analysis**: we evaluate how the benchmark size changes as we vary the strength of association between individual words and gender.\n",
    "3. **Fairness metric**: we compute the fairness metric for the different models. This includes computing both the fairness metric to a fixed threshold and the AUC. \n",
    "4. **Fairness metric 2.0**: since our fairness metric focuses on the pct of \"neutral\" models, we aim to quantify whether the models are placing the remaining probability equally on male-female examples or whether they mostly place the probabilities on one instantiation of the templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Data Loading: Load PMI difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1feeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GROUP_PAIRED_WORDLIST, FEMALE_WORDS, MALE_WORDS, get_pmi_diff, get_gender_pairs_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/extra/ucinlp1/cbelem/projects/bias-diagnosis\"\n",
    "\n",
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/word2gender_pmi_PILE.csv\", index_col=0)\n",
    "print(len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we may want to perform some correlation with other gendered words\n",
    "# we also define the PMI diff between words and other gendered word pairs\n",
    "GENDER_PAIRS, GENDER_PAIRS_NUM_WORDS = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# compute PMI diff used in the main paper\n",
    "# ----------------------------------------------------------------------------\n",
    "# Most analysis will focus on the pmi_diff(she, he)\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a049",
   "metadata": {},
   "source": [
    "## 2. Loading data - Load model scores for the different datasets\n",
    "\n",
    "Say, PMI_DIFF(w, she, he), let us now compute the pmi of the words used for each of the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"..\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"USE-5\": glob.glob(f\"{BASE_DIR}/results-words5/final-results/*__scores__*.csv\"),\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winobias*__scores__*.csv\"),\n",
    "    \"Winogender\": glob.glob(f\"{BASE_DIR}/results-baselines/final-results/*Winogender*__scores__*.csv\"),\n",
    "    # \"StereoSet\": glob.glob(f\"../results-baselines/final-results/*StereoSet*__scores__*.csv\"),\n",
    "    # We define this ordering so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"USE-10\": glob.glob(f\"{BASE_DIR}/results-words10/final-results/*__scores__*.csv\"),\n",
    "    \"USE-20\": glob.glob(f\"{BASE_DIR}/results-words20/final-results/*__scores__*.csv\"),\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES, \"\\n\", \"-\" * 62)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                                     Validation\n",
    "# ------------------------------------------------------------------------------\n",
    "# All datasets must have exact same number of files and ordered in the same way.\n",
    "for dataset1, dataset2 in itertools.product(DATASET_NAMES, DATASET_NAMES):\n",
    "    fps1 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset1]] \n",
    "    fps2 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset2]] \n",
    "    c1, c2 = Counter(fps1), Counter(fps2)\n",
    "    assert len(c1 & c2) == len(c1), f\"Validation failed for datasets: ({dataset1}, {dataset2})\"\n",
    "\n",
    "# !! Assumption: When scoring there was no change in the ordering of the templates and therefore\n",
    "# every time we load the filepaths, we will have exactly the same ordering for all files (regardless\n",
    "# of the scoring model).\n",
    "DATASET_2_FILEPATHS = {k: sorted(v) for k, v in DATASET_2_FILEPATHS.items()}\n",
    "\n",
    "print(\" Number of files per dataset\", \"\\n\", \"-\" * 65)\n",
    "for name, files in DATASET_2_FILEPATHS.items():\n",
    "    print(\" -> \", name, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd74374",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "Transform the datasets into the canonic form:\n",
    "\n",
    "1. Transform model name into its canonic form: Extract from filepath name and add it as a column to the dataset.\n",
    "3. Obtain information about model size: \n",
    "2. Obtain information about the interventions: Is the model trained on duplicated data (is_deduped=False) or non-duplicated data (is_deduped=True).\n",
    "3. Obtain information about whether the test sentence pair is natural (is_natural=True) or whether is unnatural for one of the variants in the pair (is_natural=False)\n",
    "4. Obtain information about the model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonic_model_name(model_name: str) -> str:\n",
    "    if \"EleutherAI__\" in model_name:\n",
    "        model_name = model_name.replace(\"EleutherAI__\", \"\")\n",
    "    elif \"facebook__\" in model_name:\n",
    "        model_name = model_name.replace(\"facebook__\", \"\")\n",
    "    elif \"70b-hf__snapshots\" in model_name:\n",
    "        model_name = \"llama-2-70b\"\n",
    "    elif \"llama\" in model_name:\n",
    "        ix = model_name.index(\"llama\")\n",
    "        model_name = model_name[ix:].replace(\"__hf_models__\", \"-\")\n",
    "        model_name = model_name.replace(\"B\", \"b\")\n",
    "    elif \"mosaicml__\" in model_name:\n",
    "        model_name = model_name.replace(\"mosaicml__\", \"\")\n",
    "    elif \"allenai__\" in model_name:\n",
    "        model_name = model_name.replace(\"allenai__\", \"\")\n",
    "    elif \"mistralai__\" in model_name:\n",
    "        model_name = model_name.replace(\"mistralai__\", \"\")\n",
    "    if \"deduped\" in model_name:\n",
    "        model_name = model_name.replace(\"-deduped\", \" (D)\")\n",
    "    return model_name\n",
    "\n",
    "\n",
    "def get_model_size(canonic_name: str) -> int:\n",
    "    import re \n",
    "    val = re.search(r\"(\\d+(\\.\\d+)?)(b|B|m|M)\", canonic_name)[0]\n",
    "    const = 1_000 if val[-1] in (\"b\", \"B\") else 1        \n",
    "    return float(val[:-1]) * const\n",
    "        \n",
    "    \n",
    "def get_model_family(model_name: str) -> str:\n",
    "    \"\"\"Collects information about the model family\"\"\"\n",
    "    if \"pythia\" in model_name:\n",
    "        return \"pythia\"\n",
    "    elif \"opt\" in model_name:\n",
    "        return \"opt\"\n",
    "    elif \"mpt\" in model_name:\n",
    "        return \"mpt\"\n",
    "    elif \"llama\" in model_name:\n",
    "        return \"llama2\"\n",
    "    elif \"gpt\" in model_name:\n",
    "        return \"gpt-j\"\n",
    "\n",
    "    \n",
    "def is_deduped(model_name: str) -> bool:\n",
    "    \"\"\"Collect information about whether the model was trained on deduplicated data.\"\"\"\n",
    "    return True if '-deduped' in model_name else False\n",
    "    \n",
    "\n",
    "def is_intervention(model_name: str) -> bool:\n",
    "    \"\"\"Collect information about whether the model was trained on deduplicated data \n",
    "    and with gender bias intervention.\n",
    "    \"\"\"\n",
    "    return True if '-intervention' in model_name else False\n",
    "\n",
    "\n",
    "def remove_unnatural_examples(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter out unnatural examples from the provided dataframe.\n",
    "    \n",
    "    Natural test sentence pairs are those for which ChatGPT\n",
    "    indicates that both sentence variants (regardless of gender)\n",
    "    are both likely to occur. If one of them is unlikely (as per\n",
    "    ChatGPT prediction) then we will deem the whole test sentence\n",
    "    pair unnatural and remove it.\n",
    "    \n",
    "    The proposed datasets were generated from scratch and therefore\n",
    "    will be the only ones with this column. The WinoBias and Winogender\n",
    "    have no such information, since we know by definition that both\n",
    "    completions of the sentences are both likely.\n",
    "    \"\"\"\n",
    "    if \"is_natural\" in df.columns:\n",
    "        return df[df[\"is_natural\"]].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def read_filepath(fp: str, dataset: str, filter_unnatural: bool) -> pd.DataFrame:\n",
    "    # print(fp)\n",
    "    df = pd.read_csv(fp)\n",
    "    # df has \"model\" information, with the fully qualified name (including company name)\n",
    "    \n",
    "    # add dataset name to dataframe\n",
    "    df[\"dataset\"] = dataset\n",
    "    # add boolean identifying whether model was trained on deduplicated data\n",
    "    df[\"is_deduped\"] = df[\"model\"].apply(is_deduped)\n",
    "    # add boolean indentifying whether model was trained with gender swap\n",
    "    df[\"is_intervention\"] = df[\"model\"].apply(is_intervention)\n",
    "    # add canonic name (no company name, with size info)\n",
    "    df[\"orig_model_name\"] = df[\"model\"]\n",
    "    df[\"model\"] = df[\"model\"].apply(canonic_model_name)\n",
    "    # add model size (as a float)\n",
    "    df[\"model_size\"] = df[\"model\"].apply(get_model_size)\n",
    "    # add model family\n",
    "    df[\"model_family\"] = df[\"model\"].apply(get_model_family)\n",
    "\n",
    "    # add information about whether templates are likely or unlikely\n",
    "    if filter_unnatural:\n",
    "        bef = len(df)\n",
    "        df = remove_unnatural_examples(df)\n",
    "        print(f\"Filtered {len(df) - bef} unnatural, removed from\", dataset)\n",
    "        if \"is_natural\" in df.columns:\n",
    "            print(df[\"is_natural\"].value_counts())\n",
    "            print(df[\"likely_under\"].value_counts())\n",
    "    df = df.reset_index(names=[\"orig_index\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Mapping from dataset name to the file dataframes\n",
    "DATASET_2_FILES = defaultdict(list)\n",
    "\n",
    "# Read each individual filepath, creating an association <str, list<dataframe>>.\n",
    "# every str should have a list of the same size.\n",
    "\n",
    "# To test the impact of ommiting the unnaturalness check, CHANGE THE VALUE BELOW TO FALSE\n",
    "FILTER_UNNATURAL = True\n",
    "# ------------------------------ ------------------------------ ------------------------------\n",
    "DATASET_2_FILES = {\n",
    "    dataset: [read_filepath(fp, dataset, filter_unnatural=FILTER_UNNATURAL) for fp in sorted(fps)]\n",
    "    for dataset, fps in DATASET_2_FILEPATHS.items()\n",
    "}\n",
    "\n",
    "# Merge all the dataframes into a single big dataframe that contains the information of all models\n",
    "# for each dataset. We've created a original index to keep track of the unique sentences.\n",
    "# Sort the files per (model, orig_index)\n",
    "DATASET_2_FILES = {\n",
    "    dataset: pd.concat(dfs).sort_values([\"model\", \"orig_index\"]).reset_index(drop=True)\n",
    "    for dataset, dfs in DATASET_2_FILES.items()\n",
    "}\n",
    "\n",
    "# Number of models being evaluated \n",
    "NUM_EVAL_MODELS = []\n",
    "MODELS = []\n",
    "for dataset, df  in DATASET_2_FILES.items():\n",
    "    print(dataset, df[\"model\"].nunique())\n",
    "    MODELS.extend(df[\"model\"].unique())\n",
    "    NUM_EVAL_MODELS.append(df[\"model\"].nunique())\n",
    "    \n",
    "# We force the number of models to be the same across all datasets\n",
    "assert len(set(NUM_EVAL_MODELS)) == 1, \\\n",
    "    f\"Found various model sizes: {NUM_EVAL_MODELS}\"\n",
    "\n",
    "NUM_EVAL_MODELS = NUM_EVAL_MODELS[0]\n",
    "print(\"Evaluating\", NUM_EVAL_MODELS, \"models:\")\n",
    "MODELS = list(sorted(set(MODELS)))\n",
    "print(\" -\", \"\\n - \".join(MODELS))\n",
    "\n",
    "DATASET_2_FILES[\"USE-5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776380d",
   "metadata": {},
   "source": [
    "## Post processing: \n",
    "\n",
    "In this section, we will carry some processing of the templates (column \"template\").\n",
    "\n",
    "\n",
    "**1. Remove placeholders from templates** : We first remove the placeholders (e.g., \"{PRONOUN}\", \"{PRONOUN1}\", \"{PRONOUN2}\", \"{PRONOUN2}self\") from the template.\n",
    "\n",
    "**2. Remove stopwords from the templates**: We use **spacy**'s stopwords except that we add back some of the pronouns, effectively following the approach in [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "\n",
    "**3. Parse each template**: We use **spacy** tokenizer since this was what was used by [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/). While NTLK is much faster, it doesn't group together words like \"self-care\", which is treated as single word by spacy tokenizer. Therefore, we've updated the script to consider the spacy tokenization. Applying it to the whole DATASET_2_FILES[dataset] will be too time-consuming, so we will apply to the first portion of the data and then concatenate it to the dataframe.\n",
    "\n",
    "\n",
    "### Filtering\n",
    "\n",
    "\n",
    "Before applying the processing, we will first obtain the top unique templates by focusing on the subset of data of the first listed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation (!sanity check)\n",
    "# When selecting a data slice from the big dataframe\n",
    "# we must guarantee that the sentences match to one another\n",
    "# (that is necessary because the remaining of the code is relying\n",
    "# on ordering of the dataframes)\n",
    "def check_slices(dataset: pd.DataFrame, data2files: dict, models: List[str]):\n",
    "    \"\"\"Check for the ordering of the rows in ``dataset`` correspond to the\n",
    "    ones in ``data2files``. Since the data2files are ordered by models,\n",
    "    we will focus on that.\"\"\"\n",
    "    slices = []\n",
    "    for model in models:\n",
    "        df = data2files[dataset]\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "        if len(slices) > 1:\n",
    "            assert np.array_equal(slices[-1][\"template\"].values, df[\"template\"].values)    \n",
    "        slices.append(df)\n",
    "        \n",
    "    \n",
    "for dataset in DATASET_NAMES:\n",
    "    print(\"Checking slices for dataset:\", dataset)\n",
    "    check_slices(dataset=dataset, data2files=DATASET_2_FILES, models=MODELS)\n",
    "    \n",
    "# -----------------------------------------------------------------------------\n",
    "# ^Note: if the check above does not throw an error, then it means that the\n",
    "# templates can stack up based on the model, so it's ok to apply the processing\n",
    "# to the first model and then create NUM_EVAL_MODEL copies of that and insert\n",
    "# in the dataframe!!\n",
    "# -----------------------------------------------------------------------------\n",
    "DATASET_2_TEMPLATES = {\n",
    "    dataset: df[df[\"model\"] == MODELS[0]][\"template\"].values.tolist()\n",
    "    for dataset, df in DATASET_2_FILES.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3857b",
   "metadata": {},
   "source": [
    "### Processing (using Spacy tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "import nltk \n",
    "import re, string\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "PRONOUNS = [\"she\", \"her\", \"hers\", \"he\", \"his\", \"him\", \"himself\", \"herself\"]\n",
    "SPACY_PARSER = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"tagger\"])\n",
    "\n",
    "\n",
    "def postprocess_spacy(templates, pronouns=PRONOUNS):\n",
    "    def word_tokenize(sentence: str, pronouns: list, remove_stopwords: bool=True, remove_punct: bool=True):\n",
    "        doc = SPACY_PARSER(sentence)\n",
    "        # Extract the tokens that are not stopwords\n",
    "        tokens = [token.text for token in doc \n",
    "                  if (token.text in pronouns) or (not token.is_stop and not token.is_punct)]\n",
    "        return [t for t in tokens if len(t.strip()) > 0]\n",
    "\n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Step 1. Remove placeholders from the templates\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    # Step 2. Parse the sentence\n",
    "    templates = [word_tokenize(t, pronouns) for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "def postprocess_nltk(templates, pronouns=PRONOUNS):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    # We know that some sentences have some other references to other entities,\n",
    "    # let's keep some pronouns\n",
    "    nltk_stopwords -= set(pronouns)\n",
    "    punct = string.punctuation\n",
    "    \n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Remove pronouns first\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    \n",
    "    # Remove stopwords and punct\n",
    "    templates = [[w for w in word_tokenize(t) if w not in punct and w not in nltk_stopwords] for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "DATASET_2_CANONIC_TEMPLATES_SPACY = {}\n",
    "DATASET_2_CANONIC_TEMPLATES_NLTK = {}\n",
    "\n",
    "for dataset, templates in DATASET_2_TEMPLATES.items():\n",
    "    DATASET_2_CANONIC_TEMPLATES_SPACY[dataset] = postprocess_spacy(templates)\n",
    "    DATASET_2_CANONIC_TEMPLATES_NLTK[dataset] = postprocess_nltk(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00098f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_2_CANONIC_TEMPLATES_NLTK[\"USE-5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516756",
   "metadata": {},
   "source": [
    "## Data Analysis - Determine gender co-occurrence for each word \n",
    "\n",
    "In this section, we iterate the templates and compute the gender co-occurrence values for each sentence in the benchmarks. Optionally, you can weight the values of each word by the likelihood of being toxic or having negative sentiment. If such values are not provided, we assume each word is worth the same value of 1 unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dataframe to mapping from word to pmi diff for easy access\n",
    "WORD2PMI = PMI_DIFF[[\"word\", \"pmi_diff\"]].set_index(\"word\").to_dict()[\"pmi_diff\"]\n",
    "WORD2WEIGHTS = defaultdict(lambda: 1)\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on frequency\n",
    "## ----------------------------------------------------------------\n",
    "FREQ_WORDS = pd.read_csv(\"./pmi_file_w_counts.csv\", index_col=0)\n",
    "FREQ_WORDS[\"log_freq\"] = np.log(FREQ_WORDS[\"freq\"])\n",
    "\n",
    "## uncomment one of the lines below if you prefer weighting each word based\n",
    "## on the frequency of each individual word\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"freq\"]].set_index(\"word\").to_dict()[\"freq\"]\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"log_freq\"]].set_index(\"word\").to_dict()[\"log_freq\"]\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on toxicity/sentiment\n",
    "## ----------------------------------------------------------------\n",
    "## TODO:\n",
    "## -> Define toxicity for each word\n",
    "## -> Define sentiment polarity for each word (?)\n",
    "## Define a 1-to-1 mapping and assign the variable WORD2WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi_diff_per_sentences(\n",
    "        templates: List[List[str]],\n",
    "        word2pmi: dict,\n",
    "        word2weights: dict,\n",
    "    ) -> List[List[float]]:\n",
    "    \"\"\"Computes the PMI difference per individual token in the provided sentences.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    It assumes the templates/sentences are already provided as a list of tokens.\n",
    "    It returns two lists: the first one contains the list of pmi values for each of\n",
    "    the provided words (some tokens won't have a PMI value associated); the second\n",
    "    list contains the 1-1 mapping from word to pmi value and their weights.\n",
    "    \"\"\"\n",
    "    pmi_values = []\n",
    "    words_with_pmi = []\n",
    "    \n",
    "    for template in templates:\n",
    "        pmi = np.array([word2weights[w] * word2pmi.get(w) for w in template if word2pmi.get(w) is not None])\n",
    "        pmiwords = [{\n",
    "            \"word\": w, \n",
    "            \"pmi\": round(word2pmi.get(w), 2),\n",
    "            \"weight\": round(word2weights[w], 2),\n",
    "        } for w in template if word2pmi.get(w) is not None]\n",
    "        \n",
    "        pmi_values.append(pmi)\n",
    "        words_with_pmi.append(pmiwords)\n",
    "            \n",
    "    return pmi_values, words_with_pmi\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "PMI_PER_SENTENCES_NLTK = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_NLTK.items()\n",
    "}\n",
    "\n",
    "PMI_PER_SENTENCES_SPACY = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_SPACY.items()\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Plot the distribution of well-defined PMI values\n",
    "# ---------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 5, sharey=True, figsize=(FULL_WIDTH*2, 3))\n",
    "for i, dataset in enumerate(DATASET_NAMES):\n",
    "    # Compute the number of well-defined tokens with each of the approaches\n",
    "    nltk_len = list(map(len, PMI_PER_SENTENCES_NLTK[dataset][0]))\n",
    "    spacy_len = list(map(len, PMI_PER_SENTENCES_SPACY[dataset][0]))\n",
    "    \n",
    "    axes[i].spines[['right', 'top']].set_visible(False)\n",
    "    sns.histplot(list(nltk_len), label=\"nltk\", stat=\"probability\", alpha=0.6, binrange=(0,20), binwidth=1, ax=axes[i])\n",
    "    sns.histplot(list(spacy_len), label=\"spacy\", stat=\"probability\", alpha=0.6, binrange=(0,20), binwidth=1, ax=axes[i])\n",
    "    \n",
    "    \n",
    "    axes[i].set_title(dataset)\n",
    "    axes[i].set_xlabel(\"Num words\")\n",
    "    axes[i].set_ylabel(\"Fraction\")\n",
    "    \n",
    "    if i != 0:\n",
    "        if axes[i].get_legend() is not None:\n",
    "            axes[i].get_legend().remove()\n",
    "\n",
    "adjust(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037bd55",
   "metadata": {},
   "source": [
    "Since in general **spacy** tokenizer leads to higher pct of examples being matched with a word. We will use the **spacy** tokenized templates to conduct the analysis (it increases the coverage of the constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_PER_TEMPLATES = {}\n",
    "PMIWORDS_PER_TEMPLATES = {}\n",
    "\n",
    "# Change the PMI_PER_SENTENCES_SPACY with PMI_PER_SENTENCES_NLTK\n",
    "# to use NLTK tokenization instead.\n",
    "# for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_NLTK.items():\n",
    "for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_SPACY.items():\n",
    "    pmi_vals, words_per_pmi = pmi_per_sents_values\n",
    "    \n",
    "    PMI_PER_TEMPLATES[dataset] = pmi_vals\n",
    "    PMIWORDS_PER_TEMPLATES[dataset] = words_per_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6423c5",
   "metadata": {},
   "source": [
    "### Compute the constraint: MaxGenderPMI(s)\n",
    "\n",
    "In this section, we compute the max gender PMI value per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "\n",
    "def max_gender_pmi(templates_pmi: List[List[str]], col: str) -> List[dict]:\n",
    "    \"\"\"Compute the maximum PMI diff per sentence.\"\"\"\n",
    "    def _max_pmi(lst_pmis: List[str]) -> float:\n",
    "        if len(lst_pmis) > 0:\n",
    "            idx = np.argmax(np.abs(lst_pmis))\n",
    "            return lst_pmis[idx]\n",
    "    \n",
    "    results = []\n",
    "    for template_pmi in templates_pmi:\n",
    "        max_val = _max_pmi(template_pmi)\n",
    "        results.append({col: max_val, f\"{col}_invalid\": max_val is None, \"template_words_pmi\": template_pmi})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the max gender pmi values per sentence\n",
    "MAX_GENDER_PMI = {dataset: max_gender_pmi(templates_pmi, MAXGENDER_COL) \n",
    "                  for dataset, templates_pmi in PMI_PER_TEMPLATES.items()}\n",
    "\n",
    "MAX_GENDER_PMI_LONG = []\n",
    "for dataset, lst_value_dicts in MAX_GENDER_PMI.items():\n",
    "    for value_dict in lst_value_dicts:\n",
    "        r = {k: v for k, v in value_dict.items()}\n",
    "        r[\"dataset\"] = dataset\n",
    "        MAX_GENDER_PMI_LONG.append(r)\n",
    "\n",
    "MAX_GENDER_PMI_LONG = pd.DataFrame(MAX_GENDER_PMI_LONG)\n",
    "        \n",
    "# Adds the information to the original dataset with all models\n",
    "# originally, preserved in the variable DATASET_2_FILES\n",
    "DATASET_W_CONSTRAINTS = {dataset: pd.DataFrame(values * NUM_EVAL_MODELS)\n",
    "                  for dataset, values in MAX_GENDER_PMI.items()}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# \n",
    "#                        Dataset w/ MaxGender PMI constraint!\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "DATASET_W_CONSTRAINTS = {\n",
    "    dataset: pd.concat((DATASET_2_FILES[dataset], DATASET_W_CONSTRAINTS[dataset]), copy=True, axis=1)\n",
    "    for dataset in DATASET_NAMES\n",
    "}\n",
    "\n",
    "DATASET_W_CONSTRAINTS[DATASET_NAMES[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ab163-8129-4c11-b7a5-3a18bc577fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, df in DATASET_W_CONSTRAINTS.items():\n",
    "    df.to_csv(f\"./datasets/{dataset}-no-maxpmi-constraint.csv.gz\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be9dce-bbd0-4038-8734-a5d28ee00073",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"./datasets/USE-5-no-maxpmi-constraint.csv.gz\").likely_under.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a48a0",
   "metadata": {},
   "source": [
    "### Quick test about the necessity of unnaturalness checks\n",
    "\n",
    "In this small section, we want to get an idea of how useful the semantic filters we use with ChatGPT are useful or not. We will manually analyze examples that are not being filtered out by the $\\mathrm{MaxPMI}$ constraint (the strictest we applied in the paper), and we will analyze what examples are still left out but that were filtered by the semantic checks by ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FILTER_UNNATURAL:\n",
    "    # MAX PMI Threshold\n",
    "    pmi_threshold = 0.5\n",
    "\n",
    "    for dataset in (\"USE-5\", \"USE-10\", \"USE-20\"):\n",
    "        data = DATASET_W_CONSTRAINTS[dataset].copy()\n",
    "        # We don't want to double count, so filter by model\n",
    "        data = data[data[\"model\"] == MODELS[0]] \n",
    "        # We'll want to keep valid examples according to PMI\n",
    "        data[\"valid_by_pmi\"] = data[\"max_gender_pmi\"].apply(lambda s: -pmi_threshold <= s <= pmi_threshold )\n",
    "        # We want to check examples that are valid but deemed unnatural (if any)\n",
    "        data = data[(data[\"valid_by_pmi\"]) & (~data[\"is_natural\"])]\n",
    "        \n",
    "        print(\"Words seed words filtered out:\", list(data[\"word\"].value_counts().items())[:10])\n",
    "        data.to_csv(f\"camera_ready/analysis/unnaturalness/{dataset}_pmi_eps_{str(pmi_threshold).replace('.','')}.csv\")\n",
    "else:\n",
    "    print(\"Skipping naturalness analysis check... Since unlikely filters have already been applied...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a3f6a-2853-43ce-a1d6-90b6f8ecd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0873df1",
   "metadata": {},
   "source": [
    "**Obtain examples containing the word `word`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d841325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_with_word(\n",
    "        data_to_df: Dict[str, pd.DataFrame],\n",
    "        dataset: str,\n",
    "        word: str,\n",
    "        word2pmi: Dict[str, float]=WORD2PMI,\n",
    "    ):\n",
    "    data = data_to_df[dataset].copy()\n",
    "        \n",
    "    # Deduplicate the data (we should have the same exact number of examples for every model)\n",
    "    model = data.model.unique()[0]\n",
    "    data = data[data[\"model\"] == model]\n",
    "\n",
    "    print(f\"PMI('{word}'): {word2pmi[word]:.4f}\")\n",
    "    return data[data[\"word\"] == word].sort_values(\"max_gender_pmi\")[[\"sentence\", \"is_natural\", \"max_gender_pmi\"]]\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):    \n",
    "    display(get_examples_with_word(DATASET_W_CONSTRAINTS, \"USE-5\", \"common\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00a42a",
   "metadata": {},
   "source": [
    "**`MaxPMI` is invalid when there are sentences for which it is undefined**. \n",
    "In particular, we do not have term frequencies for stopwords. When using `nltk` tokenizer, since it differs from the tokenizer used to create the term frequencies (ie, `spacy`) there is a mismatch in some of the words tokenizations, leading to some examples not being considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_results = {dataset: df[df[f\"{MAXGENDER_COL}_invalid\"]] for dataset, df in DATASET_W_CONSTRAINTS.items()}\n",
    "invalid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2406ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the sentences w/ valid constraints\n",
    "DATASET_W_CONSTRAINTS = {d: df[~df[f\"{MAXGENDER_COL}_invalid\"]] for d, df in DATASET_W_CONSTRAINTS.items()}\n",
    "\n",
    "# Compute number of examples in each dataset (before filtering for gender co-occurring words)\n",
    "MAX_GENDER_PMI_LONG[[\"dataset\", MAXGENDER_COL]].groupby(\"dataset\").describe().T[DATASET_NAMES].style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40740e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GENDER_PMI_LONG[\"dataset_\"] = MAX_GENDER_PMI_LONG.dataset\n",
    "\n",
    "# Figure settings\n",
    "fig, ax = plt.subplots(1,1, figsize=(FULL_WIDTH, 4))\n",
    "sns.boxplot(MAX_GENDER_PMI_LONG, y=\"dataset_\", x=MAXGENDER_COL, ax=ax, color=\"lightgray\")\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "adjust(fig)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_xlabel(\"Average sentence gender co-occurrence score\")\n",
    "ax.set_ylabel(\"Dataset\")\n",
    "save_fig(fig, \"boxplot__average_sentence_maxpmi_score\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e20439",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = MAX_GENDER_PMI_LONG.groupby(\"dataset\").count()[[MAXGENDER_COL]]\n",
    "num_templates.rename({MAXGENDER_COL: \"orig_num_templates\"}, axis=1, inplace=True)\n",
    "num_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3028b",
   "metadata": {},
   "source": [
    "## Data Analysis - Filtering using $\\eta$\n",
    "\n",
    "In this section, we observe how the number of templates changes as we increase the max gender pmi difference. We observe that little to no evaluation examples remain after enforcing smaller values of $\\mathrm{MaxPMI(s)}$. Conversely, as we relax the constraint, more and more examples are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_eta_and_count_examples(\n",
    "        name_and_dataset: Dict[str, pd.DataFrame],\n",
    "        etas: List[float],\n",
    "        col: str,\n",
    "        constant: int=NUM_EVAL_MODELS,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Count the number of remaining examples after filtering every dataset in\n",
    "    for different settings of $|MaxPMI(s)| \\leq \\eta$\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    dataset_max_counts = defaultdict(lambda: 0)\n",
    "    for eta in etas:\n",
    "        for dataset, df in name_and_dataset.items():\n",
    "            assert df[\"model\"].nunique() == constant\n",
    "\n",
    "            counts = ((df[col] >= -eta) & (df[col] <= eta)).sum() / constant\n",
    "            results[\"dataset\"].append(dataset)\n",
    "            results[\"filter\"].append(eta)\n",
    "            results[\"counts\"].append(counts)\n",
    "            \n",
    "            if dataset_max_counts[dataset] < counts:\n",
    "                dataset_max_counts[dataset] = counts\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    results[\"freq\"] = results[[\"dataset\", \"counts\"]].apply(lambda x: x[\"counts\"]/(dataset_max_counts[x[\"dataset\"]]), axis=1)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "FILTERING_ETA = np.linspace(0.0, 2.5, 101)[::-1]\n",
    "FILTER_CURVES_RESULTS = filter_eta_and_count_examples(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    FILTERING_ETA,\n",
    "    MAXGENDER_COL,\n",
    "    NUM_EVAL_MODELS, \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(FULL_WIDTH, FULL_WIDTH*2/3))\n",
    "sns.lineplot(\n",
    "    FILTER_CURVES_RESULTS,\n",
    "    x=\"filter\",\n",
    "    y=\"freq\",\n",
    "    hue=\"dataset\",\n",
    "    lw=2) #set y=\"counts\" to plot absolute values instead\n",
    "\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "adjust(fig)\n",
    "\n",
    "ax.set_xlabel(\"$\\eta$\")\n",
    "ax.set_ylabel(\"Percentage of Dataset\")\n",
    "\n",
    "#plt.legend( loc='upper left', bbox_to_anchor=(1, 1.01))\n",
    "ax.legend(title=\"Dataset\", loc=\"upper left\", bbox_to_anchor=(0.56, 0.70))\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(0.5))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.20))\n",
    "\n",
    "# Add axis formatting\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "# Add grid\n",
    "ax.grid(axis='y', which=\"major\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim((0, 2))\n",
    "ax.set_ylim((0, 1))\n",
    "save_fig(fig, \"lineplot__datasetpct_vs_maxpmi\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe331f",
   "metadata": {},
   "source": [
    "## Fairness metrics - Fixed threshold & AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "# fairness col in natural log space\n",
    "FAIRNESS_COL = \"FM_logprob\"\n",
    "\n",
    "# probability space threshold\n",
    "_FAIRNESS_THRESHOLD = 1.65\n",
    "\n",
    "\n",
    "def use_log_10_base(ln_val: float) -> float:\n",
    "    \"\"\"Transforms natural log into log base 10.\"\"\"\n",
    "    return np.log10(np.exp(ln_val))\n",
    "\n",
    "\n",
    "def compute_neutralpct_fixed_threshold(dataset: pd.DataFrame, eps: float, col: str):\n",
    "    abs_col = dataset[col].apply(np.abs)\n",
    "    counts = (abs_col <= eps).sum()\n",
    "    freq = counts / len(dataset)\n",
    "    \n",
    "    return counts, freq\n",
    "\n",
    "\n",
    "def compute_neutralpct_auc(dataset: pd.DataFrame, epsilons: List[float], col: str):\n",
    "    results = defaultdict(list)\n",
    "    for eps in epsilons:\n",
    "        counts, freq = compute_neutralpct_fixed_threshold(dataset, eps, col)\n",
    "        results[\"fairness_eps\"].append(eps)\n",
    "        results[\"num_examples\"].append(counts)\n",
    "        results[\"pct_examples\"].append(freq)\n",
    "        \n",
    "    results = pd.DataFrame(results)    \n",
    "    return results, auc(results[\"fairness_eps\"], results[\"pct_examples\"])\n",
    "\n",
    "\n",
    "def compute_neutralpct(data: dict, models: List[str], datasets: List[str], epsilons: List[float], col: str, use_log10: callable=None):\n",
    "    results = []\n",
    "    results_auc = defaultdict(list)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        df = data[dataset].copy()\n",
    "        \n",
    "        for model in models:\n",
    "            df_model = df[df[\"model\"] == model].copy()\n",
    "            \n",
    "            if use_log10:\n",
    "                df_model[f\"{col}_base10\"] = df[col].apply(use_log10)\n",
    "                out, out_auc = compute_neutralpct_auc(df_model, epsilons, f\"{col}_base10\")            \n",
    "            else:\n",
    "                out, out_auc = compute_neutralpct_auc(df_model, epsilons, col)\n",
    "            \n",
    "            out[\"model\"] = model\n",
    "            out[\"dataset\"] = dataset\n",
    "            results.append(out)\n",
    "            \n",
    "            results_auc[\"dataset\"].append(dataset)\n",
    "            results_auc[\"model\"].append(model)\n",
    "            results_auc[\"auc\"].append(out_auc)\n",
    "            \n",
    "            \n",
    "    return pd.concat(results), pd.DataFrame(results_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0409c",
   "metadata": {},
   "source": [
    "**Natural logarithm base**: To report the results in natural logarithm, use the following cell. \n",
    "While earlier versions of the paper included the natural logarithm results, in the camera ready version of the paper, we decide to use the **base 10** since it is more intuitive and easy to reason about."
   ]
  },
  {
   "cell_type": "raw",
   "id": "42271608",
   "metadata": {},
   "source": [
    "FAIRNESS_THRESHOLD = np.log(_FAIRNESS_THRESHOLD)\n",
    "FAIRNESS_EPSILONS = np.linspace(0, 10, 101)\n",
    "MAX_AUC = 10\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fbb2c",
   "metadata": {},
   "source": [
    "**Base 10 logarithm**: To report the results for the camera ready version of the paper, we use the base 10, since it makes it easier to think about the meaning of the value in the plots. We stick to the default value of 1.65, such that the results found in earlier versions of the paper (eg, [paper at the NeurIPS SOLAR workshop in 2023](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=nMwgV2UAAAAJ&sortby=pubdate&citation_for_view=nMwgV2UAAAAJ:_kc_bZDykSQC)) can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(FAIR_AUC, y=\"dataset\", x=\"auc\")\n",
    "plt.axvline(MAX_AUC, ls=\"--\", color=\"black\", label=\"max auc\")\n",
    "plt.ylabel(\"Dataset\")\n",
    "plt.xlabel(\"Area under the fairness curve ($\\epsilon \\in [\" + f\"{FAIRNESS_EPSILONS[0]}, {FAIRNESS_EPSILONS[-1]}\" + \"]$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a9c42",
   "metadata": {},
   "source": [
    "### Fairness AUC table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the long table into a wide table, by extending it with the dataset names\n",
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "pd.pivot_table(FAIR_AUC, index=\"model\", values=[\"auc\"], columns=[\"dataset_\"]).style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_threshold_plots(fairthresholds, fairauc, datasetnames, models, use_exp=None):\n",
    "    models, tag = models[0], models[1]\n",
    "    \n",
    "    # For every dataset create a plot\n",
    "    for dataset in datasetnames:\n",
    "        # Obtain the subset corresponding to the desired dataset\n",
    "        ft_df = fairthresholds[fairthresholds[\"dataset\"] == dataset].copy()\n",
    "        \n",
    "        # Plot only the specified models\n",
    "        ft_df = ft_df[ft_df[\"model\"].isin(models)]\n",
    "        \n",
    "        # Obtain the AUC for that model and dataset\n",
    "        aucs = fairauc[(fairauc[\"dataset\"] == dataset) & (fairauc[\"model\"].isin(models))]\n",
    "        \n",
    "        ft_df[\"Deduplicated\"] = ft_df[\"model\"].apply(lambda x: \"(D)\" in x)\n",
    "        ft_df[\"Model\"] = ft_df[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "            \n",
    "        if all([\"pythia\" in m for m in models]):\n",
    "            ft_df[\"Model\"] = ft_df[\"Model\"].apply(lambda x: x.replace(\"pythia-\", \"\"))\n",
    "        \n",
    "        if dataset in (\"Winobias\", \"Winogender\"):\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH/2, 2))\n",
    "            ax.set_xlim((0, 5))\n",
    "\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH/2, 2))\n",
    "            ax.set_xlim((0, 5))\n",
    "\n",
    "            \n",
    "        adjust(fig)\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "        if use_exp is not None:\n",
    "            ft_df[\"fairness_eps\"] = ft_df[\"fairness_eps\"].apply(use_exp)\n",
    "\n",
    "        # Plot one line per model\n",
    "        # Plot one line using different stule but same color if the model is deduplicated\n",
    "        \n",
    "        if ft_df[\"Deduplicated\"].nunique() > 1:\n",
    "            kwargs = dict(style=\"Deduplicated\")\n",
    "        else:\n",
    "            kwargs = dict()\n",
    "        \n",
    "        sns.lineplot(ft_df, x=\"fairness_eps\", y=\"pct_examples\", hue=\"Model\", lw=2, ax=ax, **kwargs)\n",
    "        # ax.axvline(FAIRNESS_THRESHOLD, color=\"black\", alpha=0.5)\n",
    "        ax.set_title(dataset, fontsize=12)\n",
    "        ax.set_xlabel(\"threshold\", fontsize=12)\n",
    "        ax.set_ylabel(\"fairness metric\", fontsize=12)\n",
    "        ax.set_ylim((0, 1))\n",
    "        \n",
    "        ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "        ax.xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(0.20))\n",
    "\n",
    "        # Add axis formatting\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "\n",
    "        ax.grid(axis='x', which=\"major\", linewidth=1, linestyle='--', color=\"lightgray\")\n",
    "        ax.grid(axis='x', which=\"minor\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "        \n",
    "        # Legend\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(0.5, 0.9), fontsize=12)\n",
    "        save_fig(fig, f\"lineplot__{dataset}_{tag}_in_func_eps\", dpi=100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792fc8d",
   "metadata": {},
   "source": [
    "### AuFC: Pythia models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_models = [\n",
    "    'pythia-70m',\n",
    "    'pythia-70m (D)',\n",
    "    # 'pythia-2.8b',\n",
    "    # 'pythia-2.8b (D)',\n",
    "    'pythia-6.9b',\n",
    "    'pythia-6.9b (D)',\n",
    "    'pythia-12b',\n",
    "    'pythia-12b (D)',\n",
    "    # 'gpt-j-6b'\n",
    "], \"pythia\"\n",
    "\n",
    "opt_models = [\n",
    "    'opt-125m',\n",
    "    'opt-2.7b',\n",
    "    'opt-350m',\n",
    "    'opt-6.7b',\n",
    "], \"opt\"\n",
    "\n",
    "misc_models = [\n",
    "    'llama-2-13b',\n",
    "    'llama-2-7b',\n",
    "    'llama-2-70b',\n",
    "    'mpt-30b',\n",
    "    'mpt-7b',\n",
    "    \"OLMo-1B\",\n",
    "    \"OLMo-7B\",\n",
    "    \"Mistral-7B-v0.1\",\n",
    "    \"Mixtral-8x7B-v0.1\",\n",
    "], \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753aac9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment expression below if you want to plot the x axis in the probability space\n",
    "# (it assumes that fair thresholds and fair auc were previously computed in the log 10.)\n",
    "# fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models, use_exp=lambda x: 10**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c3185",
   "metadata": {},
   "source": [
    "### AuFC: OPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e692fcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, opt_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df4237",
   "metadata": {},
   "source": [
    "### AuFC: mpt * llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e9e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, misc_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157085f",
   "metadata": {},
   "source": [
    "Let us create the grid for the fairness threshold picture in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def individual_fairness_threshold_plot(fairthresholds, fairauc, dataset, models, max_auc, ax, use_exp=None, simplify=True):\n",
    "    # Obtain the subset corresponding to the desired dataset\n",
    "    ft_df = fairthresholds[fairthresholds[\"dataset\"] == dataset].copy()\n",
    "\n",
    "    # Plot only the specified models\n",
    "    ft_df = ft_df[ft_df[\"model\"].isin(models)]\n",
    "\n",
    "    # Obtain the AUC for that model and dataset\n",
    "    aucs = fairauc[(fairauc[\"dataset\"] == dataset) & (fairauc[\"model\"].isin(models))]\n",
    "\n",
    "    ft_df[\"Original\"] = ft_df[\"model\"].apply(lambda x: \"No\" if \"(D)\" in x else \"Yes\")\n",
    "    ft_df[\"Model\"] = ft_df[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "    \n",
    "    if simplify and all([\"pythia\" in m for m in models]):\n",
    "        ft_df[\"Model\"] = ft_df[\"Model\"].apply(lambda x: x.replace(\"pythia-\", \"\"))\n",
    "\n",
    "    if use_exp is not None:\n",
    "        ft_df[\"fairness_eps\"] = ft_df[\"fairness_eps\"].apply(use_exp)\n",
    "\n",
    "    \n",
    "    kwargs = {\"style\": \"Original\"} if ft_df[\"Original\"].nunique() > 1 else {}    \n",
    "    sns.lineplot(ft_df, x=\"fairness_eps\", y=\"pct_examples\", hue=\"Model\", lw=1, ax=ax, alpha=0.8, **kwargs)\n",
    "    # ax.axvline(FAIRNESS_THRESHOLD, color=\"black\", alpha=0.5)\n",
    "    ax.set_title(dataset, fontsize=15)\n",
    "    ax.set_xlabel(\"threshold\")\n",
    "    ax.set_ylabel(\"fairness metric\")\n",
    "    ax.set_xlim((0, max_auc))\n",
    "    ax.set_ylim((0, 1))\n",
    "\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(0.25))\n",
    "\n",
    "    # Add axis formatting\n",
    "    # ax.yaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "\n",
    "    ax.grid(axis='x', which=\"major\", linewidth=1, linestyle='--', color=\"lightgray\")\n",
    "    # ax.grid(axis='x', which=\"minor\", linewidth=1, linestyle=':', color=\"lightgray\")\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(0.40, 0.75), fontsize=12)\n",
    "    \n",
    "    \n",
    "# Separate plotting the data from formatting the figure\n",
    "def plot_results_fairness(ax, name, **kwargs):\n",
    "    if name == \"USE-5\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-5\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "\n",
    "    elif name == \"Winobias\":\n",
    "        individual_fairness_threshold_plot(dataset=\"Winobias\", ax=ax, max_auc=3, **kwargs)\n",
    "    elif name == \"Winogender\":\n",
    "        individual_fairness_threshold_plot(dataset=\"Winogender\", ax=ax, max_auc=3, **kwargs)\n",
    "    elif name == \"USE-10\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-10\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "    elif name == \"USE-20\":\n",
    "        individual_fairness_threshold_plot(dataset=\"USE-20\", ax=ax, max_auc=MAX_AUC, **kwargs)\n",
    "    else:\n",
    "        raise NotImplemented(f\"Unexpected plot: {name}\")\n",
    "\n",
    "    \n",
    "def make_figure(is_horizontal, plot_results, dataset_names, models, **kwargs):\n",
    "    models, tag = models\n",
    "    if is_horizontal:\n",
    "        \n",
    "        mosaic = []\n",
    "        width_ratios = []\n",
    "        for name in dataset_names:\n",
    "            mosaic.append(name); width_ratios.append(1)\n",
    "            mosaic.append(\".\"); width_ratios.append(0.2)\n",
    "            \n",
    "        if len(dataset_names) == 5:\n",
    "            width_ratios = [1, 0.2, 0.75, 0.2, 0.75, 0.2, 1, 0.2, 1, 0.2]\n",
    "            \n",
    "        fig, axd = plt.subplot_mosaic(\n",
    "            mosaic=[mosaic[:-1]],\n",
    "            gridspec_kw={\"width_ratios\": width_ratios[:-1]},\n",
    "            figsize=(FULL_WIDTH, 2),\n",
    "            sharey=True,\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        AB_gap, BC_gap = 0.2, 0.2\n",
    "\n",
    "        fig, axd = plt.subplot_mosaic(\n",
    "            mosaic=[\n",
    "                [\"A\"], \n",
    "                ['.'], \n",
    "                [\"B\"], \n",
    "                ['.'],\n",
    "                [\"C\"],\n",
    "            ],\n",
    "            gridspec_kw={\"height_ratios\": [1, AB_gap, 1, BC_gap, 1]},\n",
    "            figsize=(2, FULL_WIDTH),\n",
    "            sharey=True,\n",
    "        )\n",
    "    \n",
    "\n",
    "    adjust(fig)\n",
    "    \n",
    "    for name, ax in axd.items():\n",
    "        plot_results(ax, name, models=models,**kwargs)\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.set_xlabel(\"threshold\", fontsize=14)\n",
    "        ax.set_ylabel(\"fairness metric\", fontsize=14)\n",
    "\n",
    "        \n",
    "        if ax != axd[dataset_names[-1]]:\n",
    "            ax.legend([],[], frameon=False)\n",
    "        else:\n",
    "            ax.legend(loc=\"upper center\", ncol=1, bbox_to_anchor=(0.8, 0.95), fontsize=12)\n",
    "\n",
    "    return fig, tag\n",
    "\n",
    "\n",
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES,#\n",
    "                #dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-2:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots5__{tag}__fairness_metric_in_func_eps\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e772f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-3:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots4__ours__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[0:1] + DATASET_NAMES[-2:], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots3_ours__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d00281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in [pythia_models, opt_models, misc_models]:\n",
    "    fig, tag = make_figure(is_horizontal=True,\n",
    "                plot_results=plot_results_fairness,\n",
    "                dataset_names=DATASET_NAMES[1:3], \n",
    "                **dict(fairthresholds=FAIR_THRESHOLDS, fairauc=FAIR_AUC, models=models, simplify=False),\n",
    "    )\n",
    "    save_fig(fig, f\"lineplots2_others__{tag}__fairness_metric_in_func_eps\", dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a89d65",
   "metadata": {},
   "source": [
    "## Fairness neutrality: a preliminary analysis\n",
    "\n",
    "In our paper, we have introduced two different thresholds. In the beginning, we started by studying the correlation between $\\mathrm{MaxPMI}$ and the log-odds for each sentence, but found no obvious pattern (see plots below).\n",
    "1. The threshold on $\\mathrm{MaxPMI}$, i.e., $\\varepsilon_k$.\n",
    "2. The threshold on the fairness metric, i.e., $\\varepsilon_f$.\n",
    "\n",
    "Let us explore the correlation between the two values, that is, we aim to understand how the female-to-male logprobability ratio (i.e., the log odds) change with the maximum value $\\mathrm{MaxPMI}$. To do this, we will plot the correlation between `max_gender_pmi` and the `FM_logprob`.\n",
    "\n",
    "We will create a $\\log_{10}$ variant of `FM_logprob` so that the camera-ready version of the paper is updated with a more intuitive measure. Since observing the `FM_logprob` does not provide an obvious pattern in function of the $\\mathrm{MaxPMI}$, let us compute the fairness metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc570d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DATASET_W_CONSTRAINTS[\"Winobias\"].copy()\n",
    "df[\"FM_log10_prob\"] = df[\"FM_logprob\"].apply(use_log_10_base)\n",
    "df = df[df[\"model\"].isin((\"pythia-6.9b\",))]\n",
    "\n",
    "sns.jointplot(df, x=\"max_gender_pmi\", y=\"FM_log10_prob\", hue=\"is_deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DATASET_W_CONSTRAINTS[\"Winobias\"].copy()\n",
    "df[\"FM_log10_prob\"] = df[\"FM_logprob\"].apply(use_log_10_base)\n",
    "df = df[df[\"model\"].isin((\"pythia-6.9b\",))]\n",
    "\n",
    "sns.jointplot(df, x=\"max_gender_pmi\", y=\"FM_log10_prob\", joint_kws={\"alpha\": 0.5, \"s\": 8, \"linewidth\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DATASET_W_CONSTRAINTS[\"Winobias\"].copy()\n",
    "df[\"FM_log10_prob\"] = df[\"FM_logprob\"].apply(use_log_10_base)\n",
    "df = df[df[\"model\"].isin((\"Mistral-7B-v0.1\",))]\n",
    "\n",
    "sns.kdeplot(df, x=\"max_gender_pmi\", y=\"FM_log10_prob\", fill=True, levels=100, thresh=0, cmap=\"mako\")\n",
    "plt.show()\n",
    "sns.kdeplot(df, x=\"max_gender_pmi\", y=\"FM_log10_prob\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825993a0",
   "metadata": {},
   "source": [
    "## Fairness Neutrality, Unstereo Score (US)\n",
    "\n",
    "In this section, we aim to compute the different skews of the models for various constrained settings. \n",
    "In particular, we will compute:\n",
    "\n",
    "1. **Fairness metric**: focus on the computation of the neutral examples, i.e., the examples whose test sentence pair likelihoods are within $\\exp^{\\epsilon_f}$\n",
    "2. Difference in predicted female vs predicted male: if the sentences are not being predicted neutral, how is the model assigning the probability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD, FAIRNESS_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, operator\n",
    "\n",
    "\n",
    "def filter_data_by_col_val(data: pd.DataFrame, col: str, thres: float):\n",
    "    return data[(data[col] >= -thres) & (data[col] <= thres)]\n",
    "\n",
    "\n",
    "def is_neutral(df, col=FAIRNESS_COL, threshold: float=FAIRNESS_THRESHOLD):\n",
    "    assert 0 <= threshold <= 1\n",
    "    assert col in df.columns\n",
    "    return (df[col] >= -threshold) & (df[col] <= threshold)\n",
    "\n",
    "\n",
    "def get_skew(df, col=FAIRNESS_COL, threshold: float=FAIRNESS_THRESHOLD):\n",
    "    assert 0 <= threshold <= 1\n",
    "    assert col in df.columns\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"skew\"] = [\"neutral\"] * len(df)\n",
    "    df.loc[df[col] < -threshold, \"skew\"] = \"male\"\n",
    "    df.loc[df[col] >  threshold, \"skew\"] = \"female\"\n",
    "    return df[\"skew\"]\n",
    "\n",
    "\n",
    "def get_bins(val, max_val=100, edges=(15, 10, 5, 2.5, 1, FAIRNESS_THRESHOLD)):\n",
    "    __base_interval = pd.Interval(-edges[-1], edges[-1], closed=\"both\")\n",
    "    sign = np.sign(val)\n",
    "    threshold = edges[-1]\n",
    "\n",
    "    if sign == 0 or  -threshold <= val <= threshold:\n",
    "        return __base_interval\n",
    "\n",
    "    op = operator.gt if sign > 0 else operator.le\n",
    "    edges = [sign * max_val] + [e * sign for e in edges]\n",
    "\n",
    "    for i in range(1, len(edges)):\n",
    "        if op(val, edges[i]):\n",
    "            e1, e2 = edges[i-1], edges[i]\n",
    "            bins = (e1, e2) if sign < 0 else (e2, e1)\n",
    "            return pd.Interval(*bins, closed=\"neither\" if sign < 0 and bins[-1] == -threshold else \"right\")\n",
    "        \n",
    "\n",
    "def compute_skews_(data_files: dict, fairness_col, fairness_threshold, use_base_10: callable=None):\n",
    "    new_data_files = {}\n",
    "\n",
    "    for name, df in data_files.items():\n",
    "        df = df.copy()\n",
    "        get_fair_bins = lambda x: get_bins(val=x, max_val=100, edges=(15, 10, 5, 2.5, 1, fairness_threshold))\n",
    "        \n",
    "        if use_base_10:\n",
    "            df[f\"{fairness_col}_base10\"] = df[fairness_col].apply(use_base_10)\n",
    "            new_fairness_col = f\"{fairness_col}_base10\"\n",
    "        else:\n",
    "            new_fairness_col = fairness_col\n",
    "\n",
    "        df[f\"{new_fairness_col}_bins\"] = df[new_fairness_col].apply(get_fair_bins)\n",
    "\n",
    "        df[\"is_neutral\"] = is_neutral(df, new_fairness_col, fairness_threshold)\n",
    "        # Obtain a discrete measure of what gender does the model fairness_col, skews\n",
    "        # note: it assumes that positive values of fairness col will skew female\n",
    "        # completions; and negative values skew male completions...\n",
    "        print(new_fairness_col, fairness_threshold)\n",
    "        df[\"skew\"] = get_skew(df, new_fairness_col, fairness_threshold)\n",
    "        new_data_files[name] = df\n",
    "        \n",
    "    return new_data_files\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"Using threshold: {FAIRNESS_THRESHOLD:.4f} to compute fairness metric\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "BEFORE_FILTER = {dataset: df.copy() for dataset, df in DATASET_W_CONSTRAINTS.items()}\n",
    "\n",
    "# Use this version to use the natural logarithm\n",
    "# BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, 0.5)\n",
    "# use this version to use the base 10 results\n",
    "BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c65325",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_FILTER[\"USE-5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5375ec",
   "metadata": {},
   "source": [
    "### Neutrality and AuFC (per constrained setting)\n",
    "\n",
    "While we propose a pipeline to create benchmarks that satisfy the gender co-occurrence constraints, in our experiments we do not immediately restrict our benchmarks. The main goal being that we'd like to be able to study the effect of stricter PMI constraints. For that reason, in the following setting, we will compute the value of Neutrality and AuFC for $\\eta \\in \\{0.3, 0.5, 0.65, 0.8, 1\\}$. The stricter setup being $\\eta = 0.3$ and the least strict being $\\eta = 1$. The original unconstrained version of the dataset (stored in variable `BEFORE_FILTER[<dataset>]`) is denoted $\\eta = \\infty$ in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1053999",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_THRESHOLDS = [0.3, 0.5, 0.65, 0.8, 1.0]\n",
    "print(f\"Fairness col: '{FAIRNESS_COL}' and threshold: '{FAIRNESS_THRESHOLD}'\")\n",
    "AFTER_FILTER = {}\n",
    "# Filter out the dataset_w_constraints according to the different PMI thresholds (or \\epsilon_k)\n",
    "for pmi_threshold in PMI_THRESHOLDS:\n",
    "    AFTER_FILTER[pmi_threshold] = {\n",
    "        dataset: filter_data_by_col_val(df, col=MAXGENDER_COL, thres=pmi_threshold).copy()\n",
    "        for dataset, df in BEFORE_FILTER.items()\n",
    "    } \n",
    "\n",
    "AFTER_FILTER = {\n",
    "    filt: compute_skews_(bias_files, FAIRNESS_COL, FAIRNESS_THRESHOLD, use_base_10=use_log_10_base) for filt, bias_files in AFTER_FILTER.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292a816",
   "metadata": {},
   "source": [
    "### Manually selecting sentences for figure 1 in paper :3 \n",
    "\n",
    "We restricted ourselves to looking for a female sentence in USE-5 since it would be easier to plot in a single column paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, maxpmi_filter = \"USE-5\", 0.5\n",
    "counts = AFTER_FILTER[maxpmi_filter][dataset].groupby([\"sentence\", \"skew\"]).count()[\"orig_index\"].reset_index().rename({\"orig_index\": \"preferences\"}, axis=1)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    df = counts[(counts[\"skew\"] == \"female\") & (counts[\"preferences\"] >= 12)]\n",
    "    df = df.sort_values(\"preferences\", ascending=False).head()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts[\"sentence\"].isin([\n",
    "    # \"He became ruder as time passed.\"\n",
    "    # \"He plans adventurous vacations every year.\",\n",
    "    # \"He confidently led the discussion, heads eagerly following his every word.\" \n",
    "    # \"The flight cancelation affected him.\",\n",
    "    # \"We appreciate that she's here.\",\n",
    "    \"She disowns her toxic relationships.\"\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neutral_pct_w_std(data2files: dict):\n",
    "    results = defaultdict(list)\n",
    "    for dataset, df in data2files.items():\n",
    "        neutral_mean = df[[\"model\", \"is_neutral\"]].groupby(\"model\").mean()\n",
    "        neutral_mean *= 100\n",
    "\n",
    "        # computed as the variance of a bernoulli distribution\n",
    "        Y = neutral_mean\n",
    "\n",
    "        n = len(BEFORE_FILTER[\"USE-5\"]) / NUM_EVAL_MODELS\n",
    "        neutral_std = np.sqrt(Y/100 * (1 - Y/100) / n) * 100\n",
    "        # neutral_std = BEFORE_FILTER[\"USE-5\"][[\"model\", \"is_neutral\"]].groupby(\"model\").std()\n",
    "\n",
    "        results[\"dataset\"].extend([dataset if dataset != \"USE-5\" else \"USE-05\"] * len(neutral_mean))\n",
    "        results[\"model\"].extend(neutral_mean.reset_index()[\"model\"])\n",
    "        results[\"neutral_avg\"].extend(neutral_mean[\"is_neutral\"].values.tolist())\n",
    "        results[\"neutral_std\"].extend(neutral_std[\"is_neutral\"].tolist())\n",
    "        final_repr = \"$\" + neutral_mean[\"is_neutral\"].map('{:.2f}'.format) + \"_{\\\\pm \" + neutral_std[\"is_neutral\"].round(2).map('{:.2f}'.format) + \"}$\"\n",
    "\n",
    "        results[\"neutral_final\"].extend(final_repr.values.tolist())\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "    \n",
    "def compute_female_male_skews(data2files: dict, model_names=MODELS):\n",
    "    results = defaultdict(list)\n",
    "    for dataset, df in data2files.items():\n",
    "        pcts = df.groupby([\"model\", \"skew\"]).count()[\"template\"]\n",
    "        \n",
    "        for model in model_names:\n",
    "            model_res = pcts[model]\n",
    "            model_total = model_res.sum()\n",
    "            \n",
    "            results[\"dataset\"].append(dataset if dataset != \"USE-5\" else \"USE-05\")\n",
    "            results[\"model\"].append(model)\n",
    "            results[\"total\"].append(model_total)\n",
    "            results[\"pct_fem\"].append(model_res.get(\"female\", 0) / model_total * 100)\n",
    "            results[\"pct_mal\"].append(model_res.get(\"male\", 0) / model_total * 100)\n",
    "            results[\"counts_fem\"].append(model_res.get(\"female\", 0))\n",
    "            results[\"counts_mal\"].append(model_res.get(\"male\", 0))\n",
    "            results[\"partial_pct_mal\"].append(results[\"counts_mal\"][-1] / (results[\"counts_mal\"][-1] + results[\"counts_fem\"][-1]))\n",
    "            results[\"partial_pct_fem\"].append(1-results[\"partial_pct_mal\"][-1])\n",
    "\n",
    "            \n",
    "            pct_diff = round(results[\"pct_fem\"][-1] - results[\"pct_mal\"][-1], 2)\n",
    "            results[\"pct_fem_min_mal\"].append(f\"{pct_diff:.2f}\")\n",
    "           \n",
    "    return pd.DataFrame(results).round(2)\n",
    "\n",
    "\n",
    "def merge_results(data2files) -> pd.DataFrame:\n",
    "    return pd.merge(\n",
    "        compute_neutral_pct_w_std(data2files),\n",
    "        compute_female_male_skews(data2files),\n",
    "        on=[\"dataset\", \"model\"],\n",
    "        how=\"inner\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_BEFORE_FILTER = merge_results(BEFORE_FILTER)\n",
    "METRICS_AFTER_FILTER = {eta: merge_results(AFTER_FILTER[eta]) for eta in AFTER_FILTER.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1478c4",
   "metadata": {},
   "source": [
    "### Fraction of predicted male vs predicted female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a169f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relative_male_pref_plots(df, model_names, tag, dpi=150):\n",
    "    cat1, cat2 = [], []\n",
    "    for name in model_names:\n",
    "        if \"(D)\" in name:\n",
    "            cat2.append(name)\n",
    "        else:\n",
    "            cat1.append(name)\n",
    "    \n",
    "    df1 = df[df[\"model\"].isin((cat1))]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(FULL_WIDTH, 3))\n",
    "    ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "    sns.barplot(df1, y=\"dataset\", x=\"partial_pct_mal\", hue=\"model\", ax=ax, hue_order=cat1)\n",
    "    if len(cat2) > 0:\n",
    "        df2 = df[df[\"model\"].isin((cat2))]\n",
    "        sns.barplot(df2, y=\"dataset\", x=\"partial_pct_mal\", hue=\"model\", ax=ax, hue_order=cat2)\n",
    "        \n",
    "    ax.set_xlim((0, 1))\n",
    "\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(0.2))\n",
    "    ax.xaxis.set_major_formatter(PercentFormatter(1.0))  # 1.0 is to be treated as 100%\n",
    "    adjust(fig)\n",
    "\n",
    "    ax.set_ylabel(\"Dataset\")\n",
    "    ax.set_xlabel(\"Relative male preference\")\n",
    "\n",
    "    # hatches = itertools.cycle([' ', '///', '+', '-', 'x', '\\\\', '\\\\', '*', 'o', 'O', '.'][:len(model_names)])\n",
    "    # for i, bar in enumerate(ax.patches):\n",
    "    #     if i % 5 == 0:\n",
    "    #         hatch = next(hatches)\n",
    "    #     bar.set_hatch(hatch)\n",
    "\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.26), ncol=len(model_names)//2, fancybox=True, shadow=False, fontsize=12)\n",
    "    save_fig(fig, f\"barplots__male__relpref__{tag}\", dpi=dpi)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_models_small = [\n",
    " 'pythia-70m',\n",
    " 'pythia-70m (D)',\n",
    "    \n",
    " 'pythia-1.4b',\n",
    " 'pythia-1.4b (D)',\n",
    " #'pythia-160m',\n",
    " # 'pythia-160m (D)',\n",
    " 'pythia-2.8b',\n",
    " 'pythia-2.8b (D)',\n",
    " #'pythia-410m',\n",
    " # 'pythia-410m (D)',\n",
    "], \"pythia-small\"\n",
    "\n",
    "pythia_models_large = [\n",
    " 'gpt-j-6b',\n",
    " 'pythia-6.9b',\n",
    " 'pythia-6.9b (D)',\n",
    " 'pythia-12b',\n",
    " 'pythia-12b (D)',\n",
    "], \"pythia-large\"\n",
    "\n",
    "\n",
    "for maxpmi_eps, remain_data in METRICS_AFTER_FILTER.items():\n",
    "    for models in [pythia_models_small, pythia_models_large, opt_models, misc_models]:\n",
    "        plot_relative_male_pref_plots(remain_data, pythia_models_small[0], f\"__maxpmi{maxpmi_eps}__{pythia_models_small[1]}\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7b5c3",
   "metadata": {},
   "source": [
    "#### Plot relative preferences across models \n",
    "\n",
    "More interestingly, we want to be able to compare groups of models for the same dataset. \n",
    "- Is there any scaling law by family of models? \n",
    "- How do different models perform in different filters of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41008e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: x axis are different models, sorted by "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91dbbc",
   "metadata": {},
   "source": [
    "#### Number of examples before and after the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All examples:\")\n",
    "print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in BEFORE_FILTER.items()})\n",
    "\n",
    "\n",
    "for eps, eps_values in AFTER_FILTER.items():\n",
    "    print()\n",
    "    print(\"Number of examples after filter\", eps)\n",
    "    print({dataset: len(df) / NUM_EVAL_MODELS for dataset, df in eps_values.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53149977",
   "metadata": {},
   "source": [
    "### Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2latex(model: str):    \n",
    "    if \"pythia\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"pythia-(.+)\", r\"pyths{\\1}\", model)\n",
    "    elif \"opt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"opt-(.+)\", r\"opts{\\1}\", model)\n",
    "    elif \"mpt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"mpt-(.+)\", r\"mpts{\\1}\", model)\n",
    "    elif \"llama-2\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"llama-2-(.+)\", r\"llamas{\\1}\", model)\n",
    "    elif \"gpt-j\" in model:\n",
    "        return \"\\\\\" + \"gptj\"\n",
    "    else:\n",
    "        return model\n",
    "        \n",
    "\n",
    "def print_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    print(table.set_index(\"model\").to_latex())\n",
    "\n",
    "    \n",
    "def get_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    return table.set_index(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b575d",
   "metadata": {},
   "source": [
    "### Neutral fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"NO FILTER\")\n",
    "print(\"\\n\", \"-\" * 80, \"\\n\\n\")\n",
    "print_results(METRICS_BEFORE_FILTER, \"neutral_final\")\n",
    "\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print_results(METRICS_AFTER_FILTER[eps], \"neutral_final\")\n",
    "    print(\"-\" * 80, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16481888",
   "metadata": {},
   "source": [
    "### Create tables w/ fairness gap\n",
    "\n",
    "\n",
    "#### Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NO FILTER\")\n",
    "r = get_results(METRICS_BEFORE_FILTER, \"neutral_avg\")\n",
    "r.to_csv(\"camera_ready/table/neutral_avg__unfiltered.csv\")\n",
    "fairness_gap_tables = {\"unfiltered\": r}\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    r = get_results(METRICS_AFTER_FILTER[eps], \"neutral_avg\")\n",
    "    r.to_csv(f\"camera_ready/table/neutral_avg__filtered__{eps}.csv\")\n",
    "    fairness_gap_tables[eps] = r\n",
    "    \n",
    "    \n",
    "orig = fairness_gap_tables[\"unfiltered\"]\n",
    "delta_08 = fairness_gap_tables[0.8] - orig\n",
    "delta_065 = fairness_gap_tables[0.65] - orig\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df.index = orig.index\n",
    "assert all(df.index == delta_08.index)\n",
    "assert all(df.index == delta_065.index)\n",
    "\n",
    "for dataset in [\"USE-05\", \"Winobias\", \"Winogender\"]:\n",
    "    df.insert(len(df.columns), f\"{dataset}__Orig\", orig[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.8}\", delta_08[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.65}\", delta_065[dataset])\n",
    "    \n",
    "print(df.style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9014fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df.index = orig.index\n",
    "assert all(df.index == delta_08.index)\n",
    "assert all(df.index == delta_065.index)\n",
    "\n",
    "for dataset in [\"USE-10\", \"USE-20\"]:\n",
    "    df.insert(len(df.columns), f\"{dataset}__Orig\", orig[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.8}\", delta_08[dataset])\n",
    "    df.insert(len(df.columns), f\"{dataset}__\\delta_\" + \"{0.65}\", delta_065[dataset])\n",
    "    \n",
    "print(df.style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efea9cb",
   "metadata": {},
   "source": [
    "#### Table 2. Impact of training data deduplication at $\\eta = 0.65$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cebfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.65\n",
    "tab2 = fairness_gap_tables[eta].reset_index().copy()\n",
    "tab2 = tab2[tab2[\"model\"].apply(lambda s: s.startswith(\"\\pyths\"))]\n",
    "\n",
    "tab2_dedup_mask = tab2[\"model\"].apply(lambda s: '(D)' in s)\n",
    "# original models\n",
    "tab2_orig = tab2[~tab2_dedup_mask].sort_values(\"model\")\n",
    "tab2_orig = tab2_orig.set_index(\"model\")\n",
    "\n",
    "# deduplicate models\n",
    "tab2_dedup = tab2[tab2_dedup_mask].sort_values(\"model\")\n",
    "tab2_dedup[\"model\"] = tab2_dedup[\"model\"].apply(lambda s: s.replace(\" (D)\", \"\"))\n",
    "tab2_dedup = tab2_dedup.set_index(\"model\")\n",
    "\n",
    "assert all(tab2_dedup.index == tab2_orig.index)\n",
    "\n",
    "print((tab2_dedup - tab2_orig).style.format('{:.2f}').to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af6788",
   "metadata": {},
   "source": [
    "#### Table 3. Predicted female - predicted male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_AFTER_FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab3 = get_results(METRICS_AFTER_FILTER[0.5], \"pct_fem_min_mal\")\n",
    "print(tab3.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b37cf3",
   "metadata": {},
   "source": [
    "### pred female - pred male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(METRICS_BEFORE_FILTER, \"pct_fem_min_mal\")\n",
    "get_results(METRICS_AFTER_FILTER[0.8], \"pct_fem_min_mal\")\n",
    "get_results(METRICS_AFTER_FILTER[0.65], \"pct_fem_min_mal\")\n",
    "get_results(METRICS_AFTER_FILTER[0.5], \"pct_fem_min_mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e87887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"NO FILTER\")\n",
    "print(\"\\n\", \"-\" * 80, \"\\n\\n\")\n",
    "print_results(METRICS_BEFORE_FILTER, \"pct_fem_min_mal\")\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print_results(METRICS_AFTER_FILTER[eps], \"pct_fem_min_mal\")\n",
    "    print(\"-\" * 80, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31431b66-8365-4f3a-a3dd-20cf88845b97",
   "metadata": {},
   "source": [
    "        \n",
    "        for model in model_names:\n",
    "            model_res = pcts[model]\n",
    "            model_total = model_res.sum()\n",
    "            \n",
    "            results[\"dataset\"].append(dataset if dataset != \"USE-5\" else \"USE-05\")\n",
    "            results[\"model\"].append(model)\n",
    "            results[\"total\"].append(model_total)\n",
    "            results[\"pct_fem\"].append(model_res.get(\"female\", 0) / model_total * 100)\n",
    "            results[\"pct_mal\"].append(model_res.get(\"male\", 0) / model_total * 100)\n",
    "            results[\"counts_fem\"].append(model_res.get(\"female\", 0))\n",
    "            results[\"counts_mal\"].append(model_res.get(\"male\", 0))\n",
    "            results[\"partial_pct_mal\"].append(results[\"counts_mal\"][-1] / (results[\"counts_mal\"][-1] + results[\"counts_fem\"][-1]))\n",
    "            results[\"partial_pct_fem\"].append(1-results[\"partial_pct_mal\"][-1])\n",
    "\n",
    "            \n",
    "            pct_diff = round(results[\"pct_fem\"][-1] - results[\"pct_mal\"][-1], 2)\n",
    "            results[\"pct_fem_min_mal\"].append(f\"{pct_diff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "__d = AFTER_FILTER[0.65][\"Winobias\"]\n",
    "__dgb = __d.groupby([\"model\", \"skew\"]).count()[\"template\"].reset_index()\n",
    "\n",
    "model_names = sorted(set(__dgb.model))\n",
    "\n",
    "\n",
    "total_templates = []\n",
    "for model in model_names:\n",
    "    __dm = __dgb[__dgb[\"model\"] == model]\n",
    "    \n",
    "    # Total number of templates\n",
    "    total_templates.append(__dm[\"template\"].sum())\n",
    "    \n",
    "    if \"Mixtral\" in model or \"OLMo-7B\" in model or \"opt-6.7b\" in model or \"12b\" in model or \"gpt-j\" in model:\n",
    "        fem_count = __dm.loc[__dm[\"skew\"] == \"female\", \"template\"].item()\n",
    "        mal_count = __dm.loc[__dm[\"skew\"] == \"male\", \"template\"].item()\n",
    "        print(model, round((__dm.loc[__dm[\"skew\"] == \"neutral\", \"template\"].item()) / total_templates[-1] * 100, 2), round((fem_count - mal_count) / total_templates[-1] * 100, 2))\n",
    "    \n",
    "    \n",
    "set(total_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2PMI[\"guides\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd2c63",
   "metadata": {},
   "source": [
    "### AuFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUFC_BASE_DIR = \"./camera_ready/table/aufc\"\n",
    "\n",
    "def print_results_aufc(data_auc, filepath):\n",
    "    table = pd.pivot(data_auc, values=[\"auc\"], index=\"model\", columns=[\"dataset_\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(table_str)\n",
    "    \n",
    "    # To latex file, leveraging rendering commands for model names\n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    table_str = table.set_index(\"model\").style.format('{:.2f}').to_latex()\n",
    "    print(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12c518-01a5-4806-ba51-b70aa0b3ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31021f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a file for landing page with the different metrics.\n",
    "# Ideally, we create a different json file for every dataset\n",
    "# where json file contains for every filter/max pmi constraint the models'\n",
    "# values for a given metric.\n",
    "# -----------------------\n",
    "# Example for dataset X\n",
    "# -----------------------\n",
    "# {\n",
    "#   none: {\n",
    "#     neutral__avg: {\n",
    "#        model1: 98.32,\n",
    "#        ...\n",
    "#        modeln: ...\n",
    "#     }, \n",
    "#     neutral__std: {\n",
    "#\n",
    "#     },   \n",
    "#     aufc: {\n",
    "#\n",
    "#     },  \n",
    "#     male_rel_ratio: {\n",
    "#\n",
    "#     },      \n",
    "#   },\n",
    "#   0.5: {\n",
    "#     ...\n",
    "#   },\n",
    "#   ...\n",
    "# }\n",
    "# ---------------------------------------------------------------\n",
    "METRICS_FOR_LANDING_PAGE = {name: {} for name in DATASET_NAMES}\n",
    "\n",
    "neutral__avg = {None: compute_female_male_skews(BEFORE_FILTER)}\n",
    "neutral__std = {None: compute_neutral_pct_w_std(BEFORE_FILTER)}\n",
    "\n",
    "for eps in AFTER_FILTER.keys():\n",
    "    neutral__avg[eps] = compute_female_male_skews(AFTER_FILTER[eps])\n",
    "    neutral__std[eps] = compute_neutral_pct_w_std(AFTER_FILTER[eps])\n",
    "\n",
    "    \n",
    "fair_auc_landing_page = {None: compute_neutralpct(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")[1]}\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    _, fair_auc = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    fair_auc_landing_page[eps] = fair_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d6de9-ef5f-47a6-91cc-59eaa6617d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4815a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD = np.log10(_FAIRNESS_THRESHOLD)\n",
    "print(FAIRNESS_THRESHOLD)\n",
    "MAX_AUC = 6\n",
    "FAIRNESS_EPSILONS = np.linspace(0, MAX_AUC, 101)\n",
    "\n",
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    MODELS,\n",
    "    DATASET_NAMES,\n",
    "    FAIRNESS_EPSILONS,\n",
    "    FAIRNESS_COL,\n",
    "    use_log10=use_log_10_base,\n",
    ")\n",
    "\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"-\" * 80, \"\\n\")\n",
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/unfiltered.tex\")\n",
    "\n",
    "\n",
    "for eps, df in AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"USE-5\" else \"USE-05\")\n",
    "    print_results_aufc(FAIR_AUC, f\"{AUFC_BASE_DIR}/filter_{str(eps).replace('.', '')}.tex\")\n",
    "    fair_auc_landing_page[eps] = FAIR_AUC\n",
    "    # Uncomment these lines for drawing fairness plots\n",
    "    # fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models)\n",
    "    # fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, misc_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12cc1d-94ce-4ffb-b281-750b0a95151a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASET_NAMES:\n",
    "    # fairness_neutral__avg\n",
    "    eps_results = {}\n",
    "    for eps in [None] + PMI_THRESHOLDS:\n",
    "        results = {}\n",
    "        neutral_subset = neutral__std[eps]\n",
    "        # print(dataset, eps,neutral[\"dataset\"].unique())\n",
    "        neutral_subset = neutral_subset[neutral_subset[\"dataset\"] == (\"USE-05\" if dataset == \"USE-5\" else dataset)].drop(\"neutral_final\", axis=1)\n",
    "\n",
    "        results[\"neutral__avg\"] = {}\n",
    "        results[\"neutral__std\"] = {}\n",
    "        for i, row in neutral_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"neutral__avg\"][model] = row[\"neutral_avg\"]\n",
    "            results[\"neutral__std\"][model] = row[\"neutral_std\"]\n",
    "    \n",
    "        ## AUFC \n",
    "        results[\"aufc\"] = {}\n",
    "        fair_auc = fair_auc_landing_page[eps]\n",
    "        aufc_subset = fair_auc[fair_auc.dataset == dataset]\n",
    "        for i, row in aufc_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"aufc\"][model] = row[\"auc\"]\n",
    "        \n",
    "        ## Male Relative ratio\n",
    "        male_fem_subset = neutral__avg[eps]\n",
    "        male_fem_subset = male_fem_subset[male_fem_subset.dataset == (\"USE-05\" if dataset == \"USE-5\" else dataset)]\n",
    "        results[\"male_rel_ratio\"] = {}\n",
    "        results[\"num_examples_nonneutral\"] = {}\n",
    "        results[\"num_examples\"] = male_fem_subset[\"total\"].unique().item()\n",
    "        for i, row in male_fem_subset.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            results[\"male_rel_ratio\"][model] = row[\"partial_pct_mal\"]\n",
    "            results[\"num_examples_nonneutral\"][model] = row[\"counts_fem\"] + row[\"counts_mal\"]\n",
    "        if eps == None:\n",
    "            eps = \"unconstrained\"\n",
    "        eps_results[eps] = results\n",
    "        \n",
    "    METRICS_FOR_LANDING_PAGE[dataset] = eps_results\n",
    "    import json\n",
    "    with open(f\"./camera_ready/landing-page/{dataset}.json\", \"w\") as f:\n",
    "        json.dump(eps_results, f, sort_keys=False, indent=2)\n",
    "\n",
    "\n",
    "with open(f\"./camera_ready/landing-page/all_datasets.json\", \"w\") as f:\n",
    "    json.dump(METRICS_FOR_LANDING_PAGE, f, sort_keys=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_fem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956cb8e2",
   "metadata": {},
   "source": [
    "## Sampling pictures for the images\n",
    "\n",
    "In this section, we select the examples for the paper, including the ones in the Figures 1-2, as well as the ones listed in the appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_examples(data2files, min_eps=0, max_eps=None):\n",
    "    results = {}\n",
    "    \n",
    "    for dataset, df in data2files.items():\n",
    "        df = df[~df[[\"sentence\", \"model\"]].duplicated()]\n",
    "        if min_eps == 0 :\n",
    "            mask = (df[MAXGENDER_COL].abs() >= min_eps)\n",
    "        else:\n",
    "            mask = (df[MAXGENDER_COL].abs() > min_eps)\n",
    "        \n",
    "        if max_eps is not None:\n",
    "            mask &= (df[MAXGENDER_COL].abs() <= max_eps)\n",
    "        \n",
    "        try:\n",
    "            results[dataset] = df[mask].groupby([\"word\", \"target_word\", \"template\", \"skew\"]).count()[[\"orig_index\"]]\n",
    "        except:\n",
    "            results[dataset] = df[mask].groupby([\"word\", \"template\", \"skew\"]).count()[[\"orig_index\"]]\n",
    "        \n",
    "        results[dataset].reset_index(inplace=True)\n",
    "        results[dataset].rename({\"orig_index\": \"model_votes\"}, axis=1, inplace=True)    \n",
    "    return results\n",
    "\n",
    "\n",
    "EXAMPLES_050 = get_analysis_examples(AFTER_FILTER[0.5], 0, 0.5)\n",
    "EXAMPLES_065 = get_analysis_examples(AFTER_FILTER[0.65], 0.5, 0.65)\n",
    "EXAMPLES_1 = get_analysis_examples(AFTER_FILTER[1.00], 0.65, 1)\n",
    "EXAMPLES_1plus = get_analysis_examples(BEFORE_FILTER, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0dd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES_050[\"USE-5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97330c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = \"USE-20\"\n",
    "k = 40\n",
    "d = EXAMPLES_050[dataset]\n",
    "d[(d[\"skew\"] == \"female\") & (d[\"model_votes\"] >= 12)].sample(n=k, random_state=812313)[\"template\"].head(k).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b6ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = EXAMPLES_050[dataset]\n",
    "d[(d[\"skew\"] == \"male\") & (d[\"model_votes\"] >= 12)].sample(n=k, random_state=812313)[\"template\"].head(k).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c48017",
   "metadata": {},
   "source": [
    "### Small scale human annotation \n",
    "\n",
    "In this section, we sample a set of examples from UnStereoEval (USE) datasets to carry some small scale data analysis. We select a sample of datapoints covering different MaxPMI intervals to test the hypothesis that the MaxPMI is a good heuristic for filtering out examples with gender connotated words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(data2files, n=300, seed=98283):\n",
    "    results = []\n",
    "    for dataset, df in data2files.items():\n",
    "        if not dataset.startswith(\"Wino\"):\n",
    "            # Get unique templates\n",
    "            df_sampled = df.sort_values([\"template\", \"model_votes\"], ascending=False).groupby(\"template\").head(1)\n",
    "            # Get 50 random samples\n",
    "            df_sampled = df_sampled.sample(n, random_state=seed, replace=False)\n",
    "            df_sampled[\"dataset\"] = dataset\n",
    "        \n",
    "            results.append(df_sampled)\n",
    "        \n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample(EXAMPLES_050).to_csv(\"./annotate_0_to_050_alldata_300each.csv\")\n",
    "get_sample(EXAMPLES_065).to_csv(\"./annotate_050_to_065_alldata_300each.csv\")\n",
    "get_sample(EXAMPLES_1).to_csv(\"./annotate_065_to_1_alldata_300each.csv\")\n",
    "get_sample(EXAMPLES_1plus).to_csv(\"./annotate_1plus_alldata_300each.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES_050[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
