{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "\n",
    "import itertools, warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db7679",
   "metadata": {},
   "source": [
    "## Evaluating sentence-level differences between the different benchmarks\n",
    "\n",
    "\n",
    "In this notebook, we will focus on trying to characterize the differences between the different benchmarks.\n",
    "Before we delve into the analysis, let us define the properties we're interested in this work:\n",
    "1. **words** that may (or not) be associated with gender, deemed *gender co-occurring words*; \n",
    "2. whether the **sentences are *neutral* or not**, irrespective of whether the words are associated with gender or not. We dub these sentences - *gender-invariant sentences*.\n",
    "\n",
    "\n",
    "For 2., we have curated a set of prompts and conducted human evaluation to validate our claims. That is, we've characterized the distribution and showed that both pronouns fit. This validation verifies for our datasets as well as the coreference datasets, since they are created to be neutral by default.\n",
    "\n",
    "\n",
    "We conjecture that by means of our prompts, the generated sentences are gender-invariant. Hence, this notebook will carry the evaluation for **1.**, i.e., as we vary the strength of association between gender and occurring words.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The remaining of the notebook is organized as follows: \n",
    "\n",
    "1. **Data loading**: we load both PMI difference values and the scores given by the models to each test sentence pair.\n",
    "2. **Data analysis**: we evaluate how the benchmark size changes as we vary the strength of association between individual words and gender.\n",
    "3. **Fairness metric**: we compute the fairness metric for the different models. This includes computing both the fairness metric to a fixed threshold and the AUC. \n",
    "4. **Fairness metric 2.0**: since our fairness metric focuses on the pct of \"neutral\" models, we aim to quantify whether the models are placing the remaining probability equally on male-female examples or whether they mostly place the probabilities on one instantiation of the templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194ffa5",
   "metadata": {},
   "source": [
    "## 1. Data Loading: Load PMI difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1feeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GROUP_PAIRED_WORDLIST, FEMALE_WORDS, MALE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ee5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/extra/ucinlp1/cbelem/projects/bias-diagnosis\"\n",
    "\n",
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/word2gender_pmi_PILE.csv\", index_col=0)\n",
    "print(len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_diff(df: pd.DataFrame, col1: str, col2: str, clip: int=None, missing_val: float=0, prefix_col: str=\"pmi__\") -> pd.Series:\n",
    "    \"\"\"Obtains the PMI difference between columns col1 and col2. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "    \n",
    "    col1: str\n",
    "        The female word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "    \n",
    "    col2: str\n",
    "        The male word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "        \n",
    "    clip: int, optional\n",
    "        Positive integer, specifies the cap. If not specified, the pmi\n",
    "        difference is only computed for words that co-occur with both\n",
    "        (col1, col2). If specified, we will fill the PMI value with 0\n",
    "        (ideally it would be a very negative number). You can tweak\n",
    "        this value using 'missing_val'.\n",
    "        \n",
    "    prefix_col: str\n",
    "        The prefix anteceding the col1 and col2 in the provided dataframe.\n",
    "        In our files, we prefixes all columns with gendered lexicons using\n",
    "        the \"pmi__\" prefix.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    To replicate the values of the paper you should pass female lexicon words\n",
    "    as col1 and male lexicon words as col2.\n",
    "    \"\"\"\n",
    "    assert f\"{prefix_col}{col1}\" in df.columns, f\"column {col1} is undefined in dataframe\"\n",
    "    assert f\"{prefix_col}{col2}\" in df.columns, f\"column {col2} is undefined in dataframe\"\n",
    "    \n",
    "    if clip is None:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].dropna()\n",
    "    else:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].fillna(missing_val)\n",
    "        \n",
    "    print(f\"('{col1}', '{col2}'): {len(result)}\")\n",
    "    result[f\"pmi({col1})-pmi({col2})\"] = result[f\"{prefix_col}{col1}\"] - result[f\"{prefix_col}{col2}\"]\n",
    "    \n",
    "    if clip is not None:\n",
    "        result[f\"pmi({col1})-pmi({col2})\"].clip(lower=-clip, upper=clip, inplace=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_gender_pairs_matrix(gender_pmi_df: pd.DataFrame, parallel_terms: list, **kwargs):\n",
    "    # dataframe with all the group pairs PMI (per word)\n",
    "    # (words for which no PMI diff is define)\n",
    "    pairs = gender_pmi_df[[\"word\"]].copy().set_index(\"word\")\n",
    "    num_words = []\n",
    "\n",
    "    for fword, mword in parallel_terms:\n",
    "        try:\n",
    "            # Compute the pmi difference between fword and mword\n",
    "            d = get_pmi_diff(gender_pmi_df, fword, mword, **kwargs).set_index(\"word\")\n",
    "            # Rename to be easier to visualize\n",
    "            d = d.rename({f\"pmi({fword})-pmi({mword})\": f\"{fword}-{mword}\"}, axis=1)\n",
    "            # Number of well-defined words for each of the gender pairs\n",
    "            num_words.append((f\"{fword}-{mword}\", len(d)))\n",
    "            pairs = pairs.join(d[[f\"{fword}-{mword}\"]])\n",
    "        except:\n",
    "            print(f\"! Pair ({fword}, {mword}) doesn't exist...\")\n",
    "\n",
    "    return pairs, num_words\n",
    "\n",
    "\n",
    "# Since we may want to perform some correlation with other gendered words\n",
    "# we also define the PMI diff between words and other gendered word pairs\n",
    "GENDER_PAIRS, GENDER_PAIRS_NUM_WORDS = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# compute PMI diff used in the main paper\n",
    "# ----------------------------------------------------------------------------\n",
    "# Most analysis will focus on the pmi_diff(she, he)\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a049",
   "metadata": {},
   "source": [
    "## 2. Loading data - Load model scores for the different datasets\n",
    "\n",
    "Say, PMI_DIFF(w, she, he), let us now compute the pmi of the words used for each of the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"..\"\n",
    "\n",
    "# list all the score files per dataset\n",
    "DATASET_2_FILEPATHS = {\n",
    "    \"Ours-5\": glob.glob(f\"{BASE_DIR}/results-words5/intervention-results/*__scores__*.csv\"),\n",
    "    # Baselines below ----\n",
    "    \"Winobias\": glob.glob(f\"{BASE_DIR}/results-baselines/intervention-results/*Winobias*__scores__*.csv\"),\n",
    "    \"Winogender\": glob.glob(f\"{BASE_DIR}/results-baselines/intervention-results/*Winogender*__scores__*.csv\"),\n",
    "    # \"StereoSet\": glob.glob(f\"../results-baselines/final-results/*StereoSet*__scores__*.csv\"),\n",
    "    # We define this ordering so that we can automatically obtain the same coloring scheme as\n",
    "    # the one used for word analysis\n",
    "    \"Ours-10\": glob.glob(f\"{BASE_DIR}/results-words10/intervention-results/*__scores__*.csv\"),\n",
    "    \"Ours-20\": glob.glob(f\"{BASE_DIR}/results-words20/intervention-results/*__scores__*.csv\"),\n",
    "}\n",
    "\n",
    "DATASET_NAMES = list(DATASET_2_FILEPATHS.keys())\n",
    "print(\" Dataset names:\\n  ->\", DATASET_NAMES, \"\\n\", \"-\" * 62)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                                     Validation\n",
    "# ------------------------------------------------------------------------------\n",
    "# All datasets must have exact same number of files and ordered in the same way.\n",
    "for dataset1, dataset2 in itertools.product(DATASET_NAMES, DATASET_NAMES):\n",
    "    fps1 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset1]] \n",
    "    fps2 = [fp.rpartition(\"__scores__\")[-1] for fp in DATASET_2_FILEPATHS[dataset2]] \n",
    "    c1, c2 = Counter(fps1), Counter(fps2)\n",
    "    assert len(c1 & c2) == len(c1), f\"Validation failed for datasets: ({dataset1}, {dataset2})\"\n",
    "\n",
    "# !! Assumption: When scoring there was no change in the ordering of the templates and therefore\n",
    "# every time we load the filepaths, we will have exactly the same ordering for all files (regardless\n",
    "# of the scoring model).\n",
    "DATASET_2_FILEPATHS = {k: sorted(v) for k, v in DATASET_2_FILEPATHS.items()}\n",
    "\n",
    "print(\" Number of files per dataset\", \"\\n\", \"-\" * 65)\n",
    "for name, files in DATASET_2_FILEPATHS.items():\n",
    "    print(\" -> \", name, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd74374",
   "metadata": {},
   "source": [
    "#### Preprocess the datasets\n",
    "\n",
    "Transform the datasets into the canonic form:\n",
    "\n",
    "1. Transform model name into its canonic form: Extract from filepath name and add it as a column to the dataset.\n",
    "3. Obtain information about model size: \n",
    "2. Obtain information about the interventions: Is the model trained on duplicated data (is_deduped=False) or non-duplicated data (is_deduped=True).\n",
    "3. Obtain information about whether the test sentence pair is natural (is_natural=True) or whether is unnatural for one of the variants in the pair (is_natural=False)\n",
    "4. Obtain information about the model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonic_model_name(model_name: str) -> str:\n",
    "    if \"EleutherAI__\" in model_name:\n",
    "        model_name = model_name.replace(\"EleutherAI__\", \"\")\n",
    "    elif \"facebook__\" in model_name:\n",
    "        model_name = model_name.replace(\"facebook__\", \"\")\n",
    "    elif \"llama\" in model_name:\n",
    "        ix = model_name.index(\"llama\")\n",
    "        model_name = model_name[ix:].replace(\"__hf_models__\", \"-\")\n",
    "    elif \"mosaicml__\" in model_name:\n",
    "        model_name = model_name.replace(\"mosaicml__\", \"\")\n",
    "        \n",
    "    if \"deduped\" in model_name:\n",
    "        model_name = model_name.replace(\"-deduped\", \" (D)\")\n",
    "    return model_name\n",
    "\n",
    "\n",
    "def get_model_size(canonic_name: str) -> int:\n",
    "    import re \n",
    "    val = re.search(r\"(\\d+(\\.\\d+)?)(b|B|m|M)\", canonic_name)[0]\n",
    "    const = 1_000 if val[-1] in (\"b\", \"B\") else 1        \n",
    "    return float(val[:-1]) * const\n",
    "        \n",
    "    \n",
    "def get_model_family(model_name: str) -> str:\n",
    "    \"\"\"Collects information about the model family\"\"\"\n",
    "    if \"pythia\" in model_name:\n",
    "        return \"pythia\"\n",
    "    elif \"opt\" in model_name:\n",
    "        return \"opt\"\n",
    "    elif \"mpt\" in model_name:\n",
    "        return \"mpt\"\n",
    "    elif \"llama\" in model_name:\n",
    "        return \"llama2\"\n",
    "    elif \"gpt\" in model_name:\n",
    "        return \"gpt-j\"\n",
    "\n",
    "    \n",
    "def is_deduped(model_name: str) -> bool:\n",
    "    \"\"\"Collect information about whether the model was trained on deduplicated data.\"\"\"\n",
    "    return True if '-deduped' in model_name else False\n",
    "    \n",
    "\n",
    "def is_intervention(model_name: str) -> bool:\n",
    "    \"\"\"Collect information about whether the model was trained on deduplicated data \n",
    "    and with gender bias intervention.\n",
    "    \"\"\"\n",
    "    return True if '-intervention' in model_name else False\n",
    "\n",
    "\n",
    "def get_model_step(name: str) -> int:\n",
    "    model_name = name.rpartition(\"__step\")[-1]\n",
    "    return int(model_name)\n",
    "\n",
    "\n",
    "def get_model_intervention(name: str) -> str:\n",
    "    if \"intervention\" in name:\n",
    "        return \"intervention\"\n",
    "    elif \"deduped\" in name:\n",
    "        return \"deduped\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "\n",
    "def remove_unnatural_examples(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter out unnatural examples from the provided dataframe.\n",
    "    \n",
    "    Natural test sentence pairs are those for which ChatGPT\n",
    "    indicates that both sentence variants (regardless of gender)\n",
    "    are both likely to occur. If one of them is unlikely (as per\n",
    "    ChatGPT prediction) then we will deem the whole test sentence\n",
    "    pair unnatural and remove it.\n",
    "    \n",
    "    The proposed datasets were generated from scratch and therefore\n",
    "    will be the only ones with this column. The WinoBias and Winogender\n",
    "    have no such information, since we know by definition that both\n",
    "    completions of the sentences are both likely.\n",
    "    \"\"\"\n",
    "    if \"is_natural\" in df.columns:\n",
    "        return df[df[\"is_natural\"]].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def read_filepath(fp: str, dataset: str, filter_unnatural: bool) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fp)\n",
    "    # df has \"model\" information, with the fully qualified name (including company name)\n",
    "\n",
    "    # add dataset name to dataframe\n",
    "    df[\"dataset\"] = dataset\n",
    "    # add boolean identifying whether model was trained on deduplicated data\n",
    "    df[\"is_deduped\"] = df[\"model\"].apply(is_deduped)\n",
    "    # add boolean identifying whether it is gender swap intervention\n",
    "    df[\"is_gender_swap\"] = df[\"model\"].apply(is_intervention)\n",
    "    # categorical stating what type of intervention it is\n",
    "    df[\"intervention_type\"] = df[\"model\"].apply(get_model_intervention)\n",
    "    # model step\n",
    "    df[\"step\"] = df[\"model\"].apply(get_model_step)\n",
    "\n",
    "    # add boolean indentifying whether model was trained with gender swap\n",
    "    df[\"is_intervention\"] = df[\"model\"].apply(is_intervention)\n",
    "    # add canonic name (no company name, with size info)\n",
    "    df[\"orig_model_name\"] = df[\"model\"]\n",
    "    df[\"model\"] = df[\"model\"].apply(canonic_model_name)\n",
    "    # add model size (as a float)\n",
    "    df[\"model_size\"] = df[\"model\"].apply(get_model_size)\n",
    "    # add model family\n",
    "    df[\"model_family\"] = df[\"model\"].apply(get_model_family)\n",
    "\n",
    "    # add information about whether templates are likely or unlikely\n",
    "    if filter_unnatural:\n",
    "        #print(\"Before filtering unnatural:\", len(df))\n",
    "        df = remove_unnatural_examples(df)\n",
    "        #print(\"After filtering unnatural:\", len(df))\n",
    "    \n",
    "    df = df.reset_index(names=[\"orig_index\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Mapping from dataset name to the file dataframes\n",
    "DATASET_2_FILES = defaultdict(list)\n",
    "\n",
    "# Read each individual filepath, creating an association <str, list<dataframe>>.\n",
    "# every str should have a list of the same size.\n",
    "DATASET_2_FILES = {\n",
    "    dataset: [read_filepath(fp, dataset, filter_unnatural=True) for fp in sorted(fps)]\n",
    "    for dataset, fps in DATASET_2_FILEPATHS.items()\n",
    "}\n",
    "\n",
    "# Merge all the dataframes into a single big dataframe that contains the information of all models\n",
    "# for each dataset. We've created a original index to keep track of the unique sentences.\n",
    "# Sort the files per (model, orig_index)\n",
    "DATASET_2_FILES = {\n",
    "    dataset: pd.concat(dfs).sort_values([\"model\", \"orig_index\"]).reset_index(drop=True)\n",
    "    for dataset, dfs in DATASET_2_FILES.items()\n",
    "}\n",
    "\n",
    "# Number of models being evaluated \n",
    "NUM_EVAL_MODELS = []\n",
    "MODELS = []\n",
    "for dataset, df  in DATASET_2_FILES.items():\n",
    "    print(dataset, df[\"model\"].nunique())\n",
    "    MODELS.extend(df[\"model\"].unique())\n",
    "    NUM_EVAL_MODELS.append(df[\"model\"].nunique())\n",
    "    \n",
    "# We force the number of models to be the same across all datasets\n",
    "assert len(set(NUM_EVAL_MODELS)) == 1, \\\n",
    "    f\"Found various model sizes: {NUM_EVAL_MODELS}\"\n",
    "\n",
    "NUM_EVAL_MODELS = NUM_EVAL_MODELS[0]\n",
    "print(\"Evaluating\", NUM_EVAL_MODELS, \"models:\")\n",
    "MODELS = list(sorted(set(MODELS)))\n",
    "print(\" -\", \"\\n - \".join(MODELS))\n",
    "\n",
    "DATASET_2_FILES[\"Ours-5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776380d",
   "metadata": {},
   "source": [
    "## Post processing: \n",
    "\n",
    "In this section, we will carry some processing of the templates (column \"template\").\n",
    "\n",
    "\n",
    "**1. Remove placeholders from templates** : We first remove the placeholders (e.g., \"{PRONOUN}\", \"{PRONOUN1}\", \"{PRONOUN2}\", \"{PRONOUN2}self\") from the template.\n",
    "\n",
    "**2. Remove stopwords from the templates**: We use **spacy**'s stopwords except that we add back some of the pronouns, effectively following the approach in [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/).\n",
    "\n",
    "**3. Parse each template**: We use **spacy** tokenizer since this was what was used by [Razeghi et al 2022](https://aclanthology.org/2022.emnlp-demos.39/). While NTLK is much faster, it doesn't group together words like \"self-care\", which is treated as single word by spacy tokenizer. Therefore, we've updated the script to consider the spacy tokenization. Applying it to the whole DATASET_2_FILES[dataset] will be too time-consuming, so we will apply to the first portion of the data and then concatenate it to the dataframe.\n",
    "\n",
    "\n",
    "### Filtering\n",
    "\n",
    "\n",
    "Before applying the processing, we will first obtain the top unique templates by focusing on the subset of data of the first listed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation (!sanity check)\n",
    "# When selecting a data slice from the big dataframe\n",
    "# we must guarantee that the sentences match each other\n",
    "\n",
    "def check_slices(dataset, data2files: dict, models: List[str]):\n",
    "    slices = []\n",
    "    for model in models:\n",
    "        df = data2files[dataset]\n",
    "        df = df[df[\"model\"] == model].copy()\n",
    "        \n",
    "        if len(slices) > 1:\n",
    "            assert np.array_equal(slices[-1][\"template\"].values, df[\"template\"].values) \n",
    "            \n",
    "        slices.append(df)\n",
    "        \n",
    "    \n",
    "for dataset in DATASET_NAMES:\n",
    "    print(\"Checking slices for dataset:\", dataset)\n",
    "    check_slices(dataset=dataset, data2files=DATASET_2_FILES, models=MODELS)\n",
    "    \n",
    "# -----------------------------------------------------------------------------\n",
    "# ^Note: if the check above does not throw an error, then it means that the\n",
    "# templates can stack up based on the model, so it's ok to apply the processing\n",
    "# to the first model and then create NUM_EVAL_MODEL copies of that and insert\n",
    "# in the dataframe!!\n",
    "# -----------------------------------------------------------------------------\n",
    "DATASET_2_TEMPLATES = {\n",
    "    dataset: df[df[\"model\"] == MODELS[0]][\"template\"].values.tolist()\n",
    "    for dataset, df in DATASET_2_FILES.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3857b",
   "metadata": {},
   "source": [
    "### Processing (using Spacy tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "SPACY_PARSER = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"tagger\"])\n",
    "\n",
    "def word_tokenize(sentence: str, pronouns: list, remove_stopwords: bool=True, remove_punct: bool=True):\n",
    "    doc = SPACY_PARSER(sentence)\n",
    "    # Extract the tokens that are not stopwords\n",
    "    tokens = [token.text for token in doc \n",
    "              if (token.text in pronouns) or (not token.is_stop and not token.is_punct)]\n",
    "    return [t for t in tokens if len(t.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97827995",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRONOUNS = [\"she\", \"her\", \"hers\", \"he\", \"his\", \"him\", \"himself\", \"herself\"]\n",
    "\n",
    "\n",
    "def postprocess_spacy(templates, pronouns=PRONOUNS):\n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Step 1. Remove placeholders from teh templates\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    # Step 2. Parse the sentence\n",
    "    templates = [word_tokenize(t, pronouns) for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "def postprocess_nltk(templates, pronouns=PRONOUNS):\n",
    "    import nltk \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import string, re\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    # We know that some sentences have some other references to other entities,\n",
    "    # let's keep some pronouns\n",
    "    nltk_stopwords -= set(pronouns)\n",
    "    punct = string.punctuation\n",
    "    \n",
    "    templates = [t.lower() for t in templates]\n",
    "    # Remove pronouns first\n",
    "    templates = [t.replace(\"{pronoun2}self\", \"\") for t in templates]\n",
    "    templates = [re.sub(r\"\\{pronoun([0-2]{1})?\\}\", \"\", t) for t in templates]\n",
    "    \n",
    "    # Remove stopwords and punct\n",
    "    templates = [[w for w in word_tokenize(t) if w not in punct and w not in nltk_stopwords] for t in templates]\n",
    "    return templates\n",
    "\n",
    "\n",
    "DATASET_2_CANONIC_TEMPLATES_SPACY = {}\n",
    "DATASET_2_CANONIC_TEMPLATES_NLTK = {}\n",
    "\n",
    "for dataset, templates in DATASET_2_TEMPLATES.items():\n",
    "    DATASET_2_CANONIC_TEMPLATES_SPACY[dataset] = postprocess_spacy(templates)\n",
    "    DATASET_2_CANONIC_TEMPLATES_NLTK[dataset] = postprocess_nltk(templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516756",
   "metadata": {},
   "source": [
    "## Data Analysis - Determine gender co-occurrence for each word \n",
    "\n",
    "In this section, we iterate the templates and compute the gender co-occurrence values for each sentence in the benchmarks. Optionally, you can weight the values of each word by the likelihood of being toxic or having negative sentiment. If such values are not provided, we assume each word is worth the same value of 1 unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dataframe to mapping from word to pmi diff for easy access\n",
    "WORD2PMI = PMI_DIFF[[\"word\", \"pmi_diff\"]].set_index(\"word\").to_dict()[\"pmi_diff\"]\n",
    "WORD2WEIGHTS = defaultdict(lambda: 1)\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on frequency\n",
    "## ----------------------------------------------------------------\n",
    "FREQ_WORDS = pd.read_csv(\"./pmi_file_w_counts.csv\", index_col=0)\n",
    "FREQ_WORDS[\"log_freq\"] = np.log(FREQ_WORDS[\"freq\"])\n",
    "\n",
    "## uncomment one of the lines below if you prefer weighting each word based\n",
    "## on the frequency of each individual word\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"freq\"]].set_index(\"word\").to_dict()[\"freq\"]\n",
    "# WORD2WEIGHTS = FREQ_WORDS[[\"word\", \"log_freq\"]].set_index(\"word\").to_dict()[\"log_freq\"]\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## Weighting words based on toxicity/sentiment\n",
    "## ----------------------------------------------------------------\n",
    "## TODO:\n",
    "## -> Define toxicity for each word\n",
    "## -> Define sentiment polarity for each word (?)\n",
    "## Define a 1-to-1 mapping and assign the variable WORD2WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pmi_diff_per_sentences(\n",
    "        templates: List[List[str]],\n",
    "        word2pmi: dict,\n",
    "        word2weights: dict,\n",
    "    ) -> List[List[float]]:\n",
    "    \"\"\"Computes the PMI difference per individual token in the provided sentences.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    It assumes the templates/sentences are already provided as a list of tokens.\n",
    "    It returns two lists: the first one contains the list of pmi values for each of\n",
    "    the provided words (some tokens won't have a PMI value associated); the second\n",
    "    list contains the 1-1 mapping from word to pmi value and their weights.\n",
    "    \"\"\"\n",
    "    pmi_values = []\n",
    "    words_with_pmi = []\n",
    "    \n",
    "    for template in templates:\n",
    "        pmi = np.array([word2weights[w] * word2pmi.get(w) for w in template if word2pmi.get(w) is not None])\n",
    "        pmiwords = [{\n",
    "            \"word\": w, \n",
    "            \"pmi\": round(word2pmi.get(w), 2),\n",
    "            \"weight\": round(word2weights[w], 2),\n",
    "        } for w in template if word2pmi.get(w) is not None]\n",
    "        \n",
    "        pmi_values.append(pmi)\n",
    "        words_with_pmi.append(pmiwords)\n",
    "            \n",
    "    return pmi_values, words_with_pmi\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "PMI_PER_SENTENCES_NLTK = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_NLTK.items()\n",
    "}\n",
    "\n",
    "PMI_PER_SENTENCES_SPACY = {dataset: \n",
    "                          compute_pmi_diff_per_sentences(templates, WORD2PMI, WORD2WEIGHTS)\n",
    "                          for dataset, templates in DATASET_2_CANONIC_TEMPLATES_SPACY.items()\n",
    "}\n",
    "\n",
    "# plot distribution\n",
    "for dataset in DATASET_NAMES:\n",
    "    # Compute the number of well-defined tokens with each of the approaches\n",
    "    nltk_len = list(map(len, PMI_PER_SENTENCES_NLTK[dataset][0]))\n",
    "    spacy_len = list(map(len, PMI_PER_SENTENCES_SPACY[dataset][0]))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(dataset)\n",
    "    sns.histplot(list(nltk_len), label=\"nltk\", stat=\"probability\", alpha=0.6, binrange=(0,20), binwidth=1)\n",
    "    sns.histplot(list(spacy_len), label=\"spacy\", stat=\"probability\", alpha=0.6, binrange=(0,20), binwidth=1)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037bd55",
   "metadata": {},
   "source": [
    "Since in general **spacy** tokenizer leads to higher pct of examples being matched with a word. We will use the **spacy** tokenized templates to conduct the analysis (it increases the coverage of the constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_PER_TEMPLATES = {}\n",
    "PMIWORDS_PER_TEMPLATES = {}\n",
    "\n",
    "# Change the PMI_PER_SENTENCES_SPACY with PMI_PER_SENTENCES_NLTK\n",
    "# to use NLTK tokenization instead.\n",
    "for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_NLTK.items():\n",
    "# for dataset, pmi_per_sents_values in PMI_PER_SENTENCES_SPACY.items():\n",
    "    pmi_vals, words_per_pmi = pmi_per_sents_values\n",
    "    \n",
    "    PMI_PER_TEMPLATES[dataset] = pmi_vals\n",
    "    PMIWORDS_PER_TEMPLATES[dataset] = words_per_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6423c5",
   "metadata": {},
   "source": [
    "### Compute the constraint: MaxGenderPMI(s)\n",
    "\n",
    "In this section, we compute the max gender PMI value per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXGENDER_COL = \"max_gender_pmi\"\n",
    "\n",
    "def max_gender_pmi(templates_pmi: List[List[str]], col: str) -> List[dict]:\n",
    "    \"\"\"Compute the maximum PMI diff per sentence.\"\"\"\n",
    "    def _max_pmi(lst_pmis: List[str]) -> float:\n",
    "        if len(lst_pmis) > 0:\n",
    "            idx = np.argmax(np.abs(lst_pmis))\n",
    "            return lst_pmis[idx]\n",
    "    \n",
    "    results = []\n",
    "    for template_pmi in templates_pmi:\n",
    "        max_val = _max_pmi(template_pmi)\n",
    "        results.append({col: max_val, f\"{col}_invalid\": max_val is None, \"template_words_pmi\": template_pmi})\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the max gender pmi values per sentence\n",
    "MAX_GENDER_PMI = {dataset: max_gender_pmi(templates_pmi, MAXGENDER_COL) \n",
    "                  for dataset, templates_pmi in PMI_PER_TEMPLATES.items()}\n",
    "\n",
    "MAX_GENDER_PMI_LONG = []\n",
    "for dataset, lst_value_dicts in MAX_GENDER_PMI.items():\n",
    "    for value_dict in lst_value_dicts:\n",
    "        r = {k: v for k, v in value_dict.items()}\n",
    "        r[\"dataset\"] = dataset\n",
    "        MAX_GENDER_PMI_LONG.append(r)\n",
    "\n",
    "MAX_GENDER_PMI_LONG = pd.DataFrame(MAX_GENDER_PMI_LONG)\n",
    "        \n",
    "# Adds the information to the original dataset with all models\n",
    "# originally, preserved in the variable DATASET_2_FILES\n",
    "DATASET_W_CONSTRAINTS = {dataset: pd.DataFrame(values * NUM_EVAL_MODELS)\n",
    "                  for dataset, values in MAX_GENDER_PMI.items()}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# \n",
    "#                        Dataset w/ MaxGender PMI constraint!\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "DATASET_W_CONSTRAINTS = {\n",
    "    dataset: pd.concat((DATASET_2_FILES[dataset], DATASET_W_CONSTRAINTS[dataset]), copy=True, axis=1)\n",
    "    for dataset in DATASET_NAMES\n",
    "}\n",
    "\n",
    "DATASET_W_CONSTRAINTS[DATASET_NAMES[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_results = {dataset: df[df[f\"{MAXGENDER_COL}_invalid\"]] for dataset, df in DATASET_W_CONSTRAINTS.items()}\n",
    "invalid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2406ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the sentences w/ valid constraints\n",
    "DATASET_W_CONSTRAINTS = {d: df[~df[f\"{MAXGENDER_COL}_invalid\"]] for d, df in DATASET_W_CONSTRAINTS.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GENDER_PMI_LONG[[\"dataset\", MAXGENDER_COL]].groupby(\"dataset\").describe().T[DATASET_NAMES].style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40740e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GENDER_PMI_LONG[\"dataset_\"] = MAX_GENDER_PMI_LONG.dataset.apply(lambda x: \"WB\" if x == \"Winobias\" else (\"WG\" if x == \"Winogender\" else x))\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "sns.boxplot(MAX_GENDER_PMI_LONG, x=\"dataset_\", y=MAXGENDER_COL)\n",
    "plt.ylim(-3, 3)\n",
    "plt.ylabel(\"Gender MaxPMIDiff\")\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e20439",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = MAX_GENDER_PMI_LONG.groupby(\"dataset\").count()[[MAXGENDER_COL]]\n",
    "num_templates.rename({MAXGENDER_COL: \"orig_num_templates\"}, axis=1, inplace=True)\n",
    "num_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3028b",
   "metadata": {},
   "source": [
    "## Data Analysis - Filtering using $\\epsilon_k$\n",
    "\n",
    "In this section, we observe how the number of templates changes as we increase the max gender pmi difference. We observe that little to no evaluation examples remain after enforcing smaller values of MaxGenderPMIDiff. Conversely, as we relax the constraint, more and more examples are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_epsilon_k(\n",
    "        data: dict,\n",
    "        epsilons: List[float],\n",
    "        col: str,\n",
    "        constant: int=NUM_EVAL_MODELS,\n",
    "    ) -> pd.DataFrame:\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    dataset_max_counts = defaultdict(lambda: 0)\n",
    "    for eps in epsilons:\n",
    "        for dataset, df in data.items():\n",
    "            counts = ((df[col] >= -eps) & (df[col] <= eps)).sum() / constant\n",
    "            results[\"dataset\"].append(dataset)\n",
    "            results[\"filter\"].append(eps)\n",
    "            results[\"counts\"].append(counts)\n",
    "            \n",
    "            if dataset_max_counts[dataset] < counts:\n",
    "                dataset_max_counts[dataset] = counts\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    results[\"freq\"] = results[[\"dataset\", \"counts\"]].apply(lambda x: x[\"counts\"]/(dataset_max_counts[x[\"dataset\"]]), axis=1)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "FILTERING_EPSILONS = np.linspace(0.10, 2.5, 101)[::-1]\n",
    "\n",
    "FILTER_CURVES_RESULTS = filter_epsilon_k(\n",
    "    DATASET_W_CONSTRAINTS,\n",
    "    FILTERING_EPSILONS,\n",
    "    MAXGENDER_COL,\n",
    "    NUM_EVAL_MODELS)\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "sns.lineplot(FILTER_CURVES_RESULTS, x=\"filter\", y=\"freq\", hue=\"dataset\", lw=1) #set y=\"counts\" to plot absolute values instead\n",
    "plt.xlabel(\"Gender co-occurrence threshold ($\\epsilon_k$)\")\n",
    "plt.ylabel(\"% of the dataset\")\n",
    "#plt.legend( loc='upper left', bbox_to_anchor=(1, 1.01))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76fe331f",
   "metadata": {},
   "source": [
    "## Fairness metrics - Fixed threshold & AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd44039",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_COL = \"FM_logprob\"\n",
    "FAIRNESS_THRESHOLD = 0.5\n",
    "FAIRNESS_EPSILONS = np.linspace(0, 10, 101)\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def compute_neutralpct_fixed_threshold(dataset: pd.DataFrame, eps: float, col: str):\n",
    "    abs_col = dataset[col].apply(np.abs)\n",
    "    counts = (abs_col <= eps).sum()\n",
    "    freq = counts / len(dataset)\n",
    "    \n",
    "    return counts, freq\n",
    "\n",
    "\n",
    "def compute_neutralpct_auc(dataset: pd.DataFrame, epsilons: List[float], col: str):\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for eps in epsilons:\n",
    "        counts, freq = compute_neutralpct_fixed_threshold(dataset, eps, col)\n",
    "        results[\"fairness_eps\"].append(eps)\n",
    "        results[\"num_examples\"].append(counts)\n",
    "        results[\"pct_examples\"].append(freq)\n",
    "        \n",
    "    results = pd.DataFrame(results)    \n",
    "    return results, auc(results[\"fairness_eps\"], results[\"pct_examples\"])\n",
    "\n",
    "\n",
    "def compute_neutralpct(data: dict, models: List[str], datasets: List[str], epsilons: List[float], col: str):\n",
    "    results = []\n",
    "    results_auc = defaultdict(list)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        df = data[dataset].copy()\n",
    "        \n",
    "        for model in models:\n",
    "            df_model = df[df[\"model\"] == model].copy()\n",
    "            out, out_auc = compute_neutralpct_auc(df_model, epsilons, col)\n",
    "            \n",
    "            out[\"model\"] = model\n",
    "            out[\"is_gender_swap\"] = is_intervention(model)\n",
    "            out[\"step\"] = get_model_step(model)\n",
    "            out[\"dataset\"] = dataset\n",
    "            results.append(out)\n",
    "            \n",
    "            results_auc[\"dataset\"].append(dataset)\n",
    "            results_auc[\"model\"].append(model)\n",
    "            results_auc[\"is_gender_swap\"].append(is_intervention(model))\n",
    "            results_auc[\"step\"].append(get_model_step(model))\n",
    "            results_auc[\"auc\"].append(out_auc)\n",
    "            \n",
    "            \n",
    "    return pd.concat(results), pd.DataFrame(results_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7697c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(DATASET_W_CONSTRAINTS, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(FAIR_AUC, x=\"dataset\", y=\"auc\")\n",
    "plt.axhline(10, ls=\"--\", color=\"black\", label=\"max auc\")\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(\"Area under the fairness curve ($\\epsilon_f \\in [0, 10]$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(FAIR_AUC, x=\"step\", y=\"auc\", hue=\"dataset\", style=\"is_gender_swap\", lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a9c42",
   "metadata": {},
   "source": [
    "### Fairness AUC table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"Ours-5\" else \"Ours-05\")\n",
    "pd.pivot_table(FAIR_AUC, index=\"model\", values=[\"auc\"], columns=[\"dataset_\"]).style.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_threshold_plots(fairthresholds, fairauc, datasetnames, models):\n",
    "    for dataset in datasetnames:\n",
    "        ft_df = fairthresholds[fairthresholds[\"dataset\"] == dataset]\n",
    "        ft_df = ft_df[ft_df[\"model\"].isin(models)]\n",
    "\n",
    "\n",
    "        aucs = fairauc[(fairauc[\"dataset\"] == dataset) & (fairauc[\"model\"].isin(models))]\n",
    "\n",
    "        ft_df[\"is_deduplicated\"] = ft_df[\"model\"].apply(lambda x: \"(D)\" in x)\n",
    "        ft_df[\"model\"] = ft_df[\"model\"].apply(lambda x: x.replace(\" (D)\", \"\"))\n",
    "        print(aucs)\n",
    "        plt.figure(figsize=(4,3), dpi=200)\n",
    "        sns.lineplot(ft_df, x=\"fairness_eps\", y=\"pct_examples\", hue=\"model\", style=\"is_deduplicated\", lw=1)\n",
    "        plt.axvline(FAIRNESS_THRESHOLD, color=\"gray\", alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.title(dataset)\n",
    "        plt.xlabel(\"fairness threshold ($\\epsilon_f$)\")\n",
    "        plt.ylabel(\"fairness metric ($\\\\tau$)\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792fc8d",
   "metadata": {},
   "source": [
    "### AuFC: Pythia models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8545330",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753aac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_models = [\n",
    "    'pythia-6.9b (D)__step143000',\n",
    "    'pythia-intervention-6.9b (D)__step143000',\n",
    "]\n",
    "\n",
    "fairness_threshold_plots(FAIR_THRESHOLDS, FAIR_AUC, DATASET_NAMES, pythia_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825993a0",
   "metadata": {},
   "source": [
    "## Fairness Neutrality\n",
    "\n",
    "In this section, we aim to compute the different skews of the models for various constrained settings. \n",
    "\n",
    "In particular, we will compute:\n",
    "\n",
    "1. Fairness metric: focus on the computation of the neutral examples, i.e., the examples whose test sentence pair likelihoods are within $\\exp^{\\epsilon_f}$\n",
    "2. Difference in predicted female vs predicted male: if the sentences are not being predicted neutral, how is the model assigning the probability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS_THRESHOLD = 0.5\n",
    "PMI_THRESHOLDS = [0.5, 0.65, 0.8, 1.0]\n",
    "\n",
    "def filter_data_by_col_val(data: pd.DataFrame, col: str, thres: float):\n",
    "    return data[(data[col] >= -thres) & (data[col] <= thres)]\n",
    "\n",
    "\n",
    "# Compute the constrained version of the file\n",
    "BEFORE_FILTER = {dataset: df.copy() for dataset, df in DATASET_W_CONSTRAINTS.items()}\n",
    "AFTER_FILTER = {}\n",
    "\n",
    "# Filter out the dataset_w_constraints according to the different PMI thresholds (or \\epsilon_k)\n",
    "for pmi_threshold in PMI_THRESHOLDS:\n",
    "    AFTER_FILTER[pmi_threshold] = {\n",
    "        dataset: filter_data_by_col_val(df, col=MAXGENDER_COL, thres=pmi_threshold).copy()\n",
    "        for dataset, df in BEFORE_FILTER.items()\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, operator\n",
    "\n",
    "\n",
    "def is_neutral(df, col=FAIRNESS_COL, threshold: float=FAIRNESS_THRESHOLD):\n",
    "    assert 0 <= threshold <= 1\n",
    "    assert col in df.columns\n",
    "    return (df[col] >= -threshold) & (df[col] <= threshold)\n",
    "\n",
    "\n",
    "def get_skew(df, col=FAIRNESS_COL, threshold: float=FAIRNESS_THRESHOLD):\n",
    "    assert 0 <= threshold <= 1\n",
    "    assert col in df.columns\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"skew\"] = [\"neutral\"] * len(df)\n",
    "    df.loc[df[col] < -threshold, \"skew\"] = \"male\"\n",
    "    df.loc[df[col] >  threshold, \"skew\"] = \"female\"\n",
    "    return df[\"skew\"]\n",
    "\n",
    "\n",
    "def get_bins(val, max_val=100, edges=(15, 10, 5, 2.5, 1, FAIRNESS_THRESHOLD)):\n",
    "    __base_interval = pd.Interval(-edges[-1], edges[-1], closed=\"both\")\n",
    "    sign = np.sign(val)\n",
    "    threshold = edges[-1]\n",
    "\n",
    "    if sign == 0 or  -threshold <= val <= threshold:\n",
    "        return __base_interval\n",
    "\n",
    "    op = operator.gt if sign > 0 else operator.le\n",
    "    edges = [sign * max_val] + [e * sign for e in edges]\n",
    "\n",
    "\n",
    "    for i in range(1, len(edges)):\n",
    "        if op(val, edges[i]):\n",
    "            e1, e2 = edges[i-1], edges[i]\n",
    "            bins = (e1, e2) if sign < 0 else (e2, e1)\n",
    "            return pd.Interval(*bins, closed=\"neither\" if sign < 0 and bins[-1] == -threshold else \"right\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def compute_skews_(data_files: dict, fairness_col, fairness_threshold):\n",
    "    # Note: This cell is going to add columns to the original dataframes in DATASET_2_FILES\n",
    "    for name, df in data_files.items():\n",
    "        get_fair_bins = lambda x: get_bins(val=x, max_val=100, edges=(15, 10, 5, 2.5, 1, fairness_threshold))\n",
    "        df[f\"{fairness_col}_bins\"] = df[fairness_col].apply(get_fair_bins)\n",
    "\n",
    "        df[\"is_neutral\"] = is_neutral(df, fairness_col, fairness_threshold)\n",
    "        # Obtain a discrete measure of what gender does the model fairness_col, skews\n",
    "        # note: it assumes that positive values of fairness col will skew female\n",
    "        # completions; and negative values skew male completions...\n",
    "        df[\"skew\"] = get_skew(df, fairness_col, fairness_threshold)\n",
    "        \n",
    "    return data_files\n",
    "\n",
    "\n",
    "BEFORE_FILTER = compute_skews_(BEFORE_FILTER, FAIRNESS_COL, FAIRNESS_THRESHOLD)\n",
    "\n",
    "AFTER_FILTER = {\n",
    "    filt: compute_skews_(bias_files, FAIRNESS_COL, FAIRNESS_THRESHOLD) for filt, bias_files in AFTER_FILTER.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5375ec",
   "metadata": {},
   "source": [
    "### Neutrality and AuFC (per constrained setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neutral_pct_w_std(data2files: dict):\n",
    "    results = defaultdict(list)\n",
    "    for dataset, df in data2files.items():\n",
    "        neutral_mean = df[[\"model\", \"is_neutral\"]].groupby(\"model\").mean()\n",
    "        neutral_mean *= 100\n",
    "\n",
    "        # computed as the variance of a bernoulli distribution\n",
    "        Y = neutral_mean\n",
    "\n",
    "        n = len(BEFORE_FILTER[\"Ours-5\"]) / NUM_EVAL_MODELS\n",
    "        neutral_std = np.sqrt(Y/100 * (1 - Y/100) / n) * 100\n",
    "        # neutral_std = BEFORE_FILTER[\"Ours-5\"][[\"model\", \"is_neutral\"]].groupby(\"model\").std()\n",
    "\n",
    "        results[\"dataset\"].extend([dataset if dataset != \"Ours-5\" else \"Ours-05\"] * len(neutral_mean))\n",
    "        results[\"model\"].extend(neutral_mean.reset_index()[\"model\"])\n",
    "        results[\"neutral_avg\"].extend(neutral_mean[\"is_neutral\"].values.tolist())\n",
    "        results[\"neutral_std\"].extend(neutral_std[\"is_neutral\"].tolist())\n",
    "        final_repr = \"$\" + neutral_mean[\"is_neutral\"].map('{:.2f}'.format) + \"_{\\\\pm \" + neutral_std[\"is_neutral\"].round(2).map('{:.2f}'.format) + \"}$\"\n",
    "\n",
    "        results[\"neutral_final\"].extend(final_repr.values.tolist())\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "        \n",
    "    \n",
    "def compute_female_male_skews(data2files: dict, model_names=MODELS):\n",
    "    results = defaultdict(list)\n",
    "    for dataset, df in data2files.items():\n",
    "        pcts = df.groupby([\"model\", \"skew\"]).count()[\"template\"]\n",
    "        \n",
    "        for model in model_names:\n",
    "            model_res = pcts[model]\n",
    "            model_total = model_res.sum()\n",
    "            \n",
    "            results[\"dataset\"].append(dataset if dataset != \"Ours-5\" else \"Ours-05\")\n",
    "            results[\"model\"].append(model)\n",
    "            results[\"total\"].append(model_total)\n",
    "            results[\"pct_fem\"].append(model_res.get(\"female\", 0) / model_total * 100)\n",
    "            results[\"pct_mal\"].append(model_res.get(\"male\", 0) / model_total * 100)\n",
    "            \n",
    "            pct_diff = round((model_res.get(\"female\",0) - model_res.get(\"male\", 0)) / model_total * 100, 2)\n",
    "            results[\"pct_fem_min_mal\"].append(f\"{pct_diff:.2f}\")\n",
    "           \n",
    "    return pd.DataFrame(results).round(2)\n",
    "\n",
    "\n",
    "def merge_results(data2files) -> pd.DataFrame:\n",
    "    return pd.merge(\n",
    "        compute_neutral_pct_w_std(data2files),\n",
    "        compute_female_male_skews(data2files),\n",
    "        on=[\"dataset\", \"model\"],\n",
    "        how=\"inner\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_BEFORE_FILTER = merge_results(BEFORE_FILTER)\n",
    "METRICS_AFTER_FILTER = {eps: merge_results(AFTER_FILTER[eps]) for eps in AFTER_FILTER.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15300cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_AFTER_FILTER[0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "{dataset: len(df) / NUM_EVAL_MODELS for dataset, df in BEFORE_FILTER.items()}\n",
    "{dataset: len(df) / NUM_EVAL_MODELS for dataset, df in AFTER_FILTER[0.5].items()}\n",
    "\n",
    "#{dataset: len(df) / NUM_EVAL_MODELS for dataset, df in AFTER_FILTER[0.65].items()}\n",
    "#{dataset: len(df) / NUM_EVAL_MODELS for dataset, df in AFTER_FILTER[0.80].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2latex(model: str):\n",
    "    if \"pythia\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"pythia-(.+)\", r\"pyths{\\1}\", model)\n",
    "    elif \"opt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"opt-(.+)\", r\"opts{\\1}\", model)\n",
    "    elif \"mpt\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"mpt-(.+)\", r\"mpts{\\1}\", model)\n",
    "    elif \"llama-2\" in model:\n",
    "        return \"\\\\\" + re.sub(r\"llama-2-(.+)\", r\"llamas{\\1}\", model)\n",
    "    elif \"gpt-j\" in model:\n",
    "        return \"\\\\\" + \"gptj\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected: {model}\")\n",
    "        \n",
    "        \n",
    "def print_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    print(table.set_index(\"model\").to_latex())\n",
    "\n",
    "    \n",
    "def get_results(data, value):\n",
    "    table = pd.pivot(data, values=[value], index=\"model\", columns=[\"dataset\"])\n",
    "    table = table.droplevel(None, axis=1).rename_axis(None, axis=1).reset_index() \n",
    "    table[\"model\"] = table[\"model\"].apply(model2latex)\n",
    "    return table.set_index(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b575d",
   "metadata": {},
   "source": [
    "### Neutral fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"NO FILTER\")\n",
    "print(\"\\n\", \"-\" * 80, \"\\n\\n\")\n",
    "print_results(METRICS_BEFORE_FILTER, \"neutral_final\")\n",
    "\n",
    "\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print_results(METRICS_AFTER_FILTER[eps], \"neutral_final\")\n",
    "    print(\"-\" * 80, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b37cf3",
   "metadata": {},
   "source": [
    "### pred female - pred male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(METRICS_BEFORE_FILTER, \"pct_fem_min_mal\")\n",
    "#get_results(METRICS_AFTER_FILTER[0.8], \"pct_fem_min_mal\")\n",
    "#get_results(METRICS_AFTER_FILTER[0.65], \"pct_fem_min_mal\")\n",
    "#get_results(METRICS_AFTER_FILTER[0.5], \"pct_fem_min_mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e87887",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 80, \"\\n\")\n",
    "print(\"NO FILTER\")\n",
    "print(\"\\n\", \"-\" * 80, \"\\n\\n\")\n",
    "print_results(METRICS_BEFORE_FILTER, \"pct_fem_min_mal\")\n",
    "\n",
    "\n",
    "\n",
    "for eps, df in METRICS_AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print_results(METRICS_AFTER_FILTER[eps], \"pct_fem_min_mal\")\n",
    "    print(\"-\" * 80, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd2c63",
   "metadata": {},
   "source": [
    "### AuFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps, df in AFTER_FILTER.items():\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    print(f\"FILTER = {eps}\")\n",
    "    print(\"-\" * 80, \"\\n\")\n",
    "    FAIR_THRESHOLDS, FAIR_AUC = compute_neutralpct(df, MODELS, DATASET_NAMES, FAIRNESS_EPSILONS, FAIRNESS_COL)\n",
    "    FAIR_AUC[\"dataset_\"] = FAIR_AUC[\"dataset\"].apply(lambda x: x if x != \"Ours-5\" else \"Ours-05\")\n",
    "    print(pd.pivot_table(FAIR_AUC, index=\"model\", values=[\"auc\"], columns=[\"dataset_\"]).style.format('{:.2f}').to_latex())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31021f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "956cb8e2",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_examples(data2files, min_eps=0, max_eps=None):\n",
    "    results = {}\n",
    "    \n",
    "    for dataset, df in data2files.items():\n",
    "        if min_eps == 0 :\n",
    "            mask = (df[MAXGENDER_COL].abs() >= min_eps)\n",
    "        else:\n",
    "            mask = (df[MAXGENDER_COL].abs() > min_eps)\n",
    "        \n",
    "        if max_eps is not None:\n",
    "            mask &= (df[MAXGENDER_COL].abs() <= max_eps)\n",
    "        \n",
    "        try:\n",
    "            results[dataset] = df[mask].groupby([\"word\", \"target_word\", \"template\", \"skew\"]).count()[[\"orig_index\"]]\n",
    "        except:\n",
    "            results[dataset] = df[mask].groupby([\"word\", \"template\", \"skew\"]).count()[[\"orig_index\"]]\n",
    "        \n",
    "        results[dataset].reset_index(inplace=True)\n",
    "        results[dataset].rename({\"orig_index\": \"model_votes\"}, axis=1, inplace=True)    \n",
    "    return results\n",
    "\n",
    "\n",
    "EXAMPLES_050 = get_analysis_examples(AFTER_FILTER[0.5], 0, 0.5)\n",
    "EXAMPLES_065 = get_analysis_examples(AFTER_FILTER[0.65], 0.5, 0.65)\n",
    "EXAMPLES_1 = get_analysis_examples(AFTER_FILTER[1.00], 0.65, 1)\n",
    "EXAMPLES_1plus = get_analysis_examples(BEFORE_FILTER, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(data2files, n=50, seed=98283):\n",
    "    results = []\n",
    "    for dataset, df in data2files.items():\n",
    "        if not dataset.startswith(\"Wino\"):\n",
    "            # Get unique templates\n",
    "            df_sampled = df.sort_values([\"template\", \"model_votes\"], ascending=False).groupby(\"template\").head(1)\n",
    "            # Get 50 random samples\n",
    "            df_sampled = df_sampled.sample(n, random_state=seed, replace=False)\n",
    "            df_sampled[\"dataset\"] = dataset\n",
    "        \n",
    "            results.append(df_sampled)\n",
    "        \n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample(EXAMPLES_050).to_csv(\"./annotate_0_to_050_alldata_50each.csv\")\n",
    "get_sample(EXAMPLES_065).to_csv(\"./annotate_050_to_065_alldata_50each.csv\")\n",
    "get_sample(EXAMPLES_1).to_csv(\"./annotate_065_to_1_alldata_50each.csv\")\n",
    "get_sample(EXAMPLES_1plus).to_csv(\"./annotate_1plus_alldata_50each.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eba01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd91fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
