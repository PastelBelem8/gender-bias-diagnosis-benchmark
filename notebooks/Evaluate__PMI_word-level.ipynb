{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"colorblind\"))\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import sys; sys.path.append(\"../code\")\n",
    "from run_experiment import readlines\n",
    "\n",
    "\n",
    "BASE_DIR = \"/extra/ucinlp1/cbelem/projects/bias-diagnosis\"\n",
    "\n",
    "GROUP_PAIRED_WORDLIST = [\n",
    "    (\"she\", \"he\"),\n",
    "    (\"her\", \"his\"),\n",
    "    (\"her\", \"him\"),\n",
    "    (\"hers\", \"his\"),\n",
    "    (\"grandmother\", \"grandfather\"),\n",
    "    (\"grandma\", \"grandpa\"),\n",
    "    (\"stepmother\", \"stepfather\"),\n",
    "    (\"stepmom\", \"stepdad\"),\n",
    "    (\"mother\", \"father\"),\n",
    "    (\"mom\", \"dad\"),\n",
    "    (\"aunt\", \"uncle\"),\n",
    "    (\"aunts\", \"uncles\"),\n",
    "    (\"mummy\", \"daddy\"),\n",
    "    (\"sister\", \"brother\"),\n",
    "    (\"sisters\", \"brothers\"),\n",
    "    (\"daughter\", \"son\"),\n",
    "    (\"daughters\", \"sons\"),\n",
    "    (\"female\", \"male\"),\n",
    "    (\"females\", \"males\"),\n",
    "    (\"feminine\", \"masculine\"),\n",
    "    (\"woman\", \"man\"),\n",
    "    (\"women\", \"men\"),\n",
    "    (\"madam\", \"sir\"),\n",
    "    (\"matriarchy\", \"patriarchy\"),\n",
    "    (\"girl\", \"boy\"),\n",
    "    (\"lass\", \"lad\"),\n",
    "    (\"girls\", \"boys\"),\n",
    "    (\"girlfriend\", \"boyfriend\"),\n",
    "    (\"girlfriends\", \"boyfriends\"),\n",
    "    (\"wife\", \"husband\"),\n",
    "    (\"wives\", \"husbands\"),\n",
    "    (\"queen\", \"king\"),\n",
    "    (\"queens\", \"kings\"),\n",
    "    (\"princess\", \"prince\"),\n",
    "    (\"princesses\", \"princes\"),\n",
    "    (\"lady\", \"lord\"),\n",
    "    (\"ladies\", \"lords\"),\n",
    "]\n",
    "# unpack the previous list into female, male\n",
    "FEMALE_WORDS, MALE_WORDS = zip(*GROUP_PAIRED_WORDLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a44b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(df, corr_method=\"kendall\", figsize=(8, 5), dpi=200, **kwargs):\n",
    "    print(corr_method)\n",
    "    corr = df.corr(corr_method)\n",
    "    mask = np.triu(corr)\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    sns.heatmap(corr, mask=mask, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee08f97",
   "metadata": {},
   "source": [
    "## Word-level Analysis\n",
    " \n",
    "In this notebook, we conduct the post-processing and analysis at a word level. \n",
    "The notebook includes two analysis: (1) PMI, (2) diversity. \n",
    "\n",
    "\n",
    "Before delving into the comparison between the target words used in each benchmark. We first perform an analysis of how different combinations of paired gendered words correlate with one another when used to induce a word ordering in terms of how strongly associated they are with each of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba05e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmi_diff(df: pd.DataFrame, col1: str, col2: str, clip: int=None, missing_val: float=0, prefix_col: str=\"pmi__\") -> pd.Series:\n",
    "    \"\"\"Obtains the PMI difference between columns col1 and col2. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "    \n",
    "    col1: str\n",
    "        The female word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "    \n",
    "    col2: str\n",
    "        The male word to use for computing the PMI. Should be one of the\n",
    "        available suffixes in the provided dataframe's columns.\n",
    "        \n",
    "    clip: int, optional\n",
    "        Positive integer, specifies the cap. If not specified, the pmi\n",
    "        difference is only computed for words that co-occur with both\n",
    "        (col1, col2). If specified, we will fill the PMI value with 0\n",
    "        (ideally it would be a very negative number). You can tweak\n",
    "        this value using 'missing_val'.\n",
    "        \n",
    "    prefix_col: str\n",
    "        The prefix anteceding the col1 and col2 in the provided dataframe.\n",
    "        In our files, we prefixes all columns with gendered lexicons using\n",
    "        the \"pmi__\" prefix.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    To replicate the values of the paper you should pass female lexicon words\n",
    "    as col1 and male lexicon words as col2.\n",
    "    \"\"\"\n",
    "    assert f\"{prefix_col}{col1}\" in df.columns, f\"column {col1} is undefined in dataframe\"\n",
    "    assert f\"{prefix_col}{col2}\" in df.columns, f\"column {col2} is undefined in dataframe\"\n",
    "    \n",
    "    if clip is None:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].dropna()\n",
    "    else:\n",
    "        result = df[[\"word\", f\"{prefix_col}{col1}\", f\"{prefix_col}{col2}\"]].fillna(missing_val)\n",
    "        \n",
    "    print(f\"('{col1}', '{col2}') pmi-defined words: {len(result)}\")\n",
    "    result[f\"pmi({col1})-pmi({col2})\"] = result[f\"{prefix_col}{col1}\"] - result[f\"{prefix_col}{col2}\"]\n",
    "    \n",
    "    if clip is not None:\n",
    "        result[f\"pmi({col1})-pmi({col2})\"].clip(lower=-clip, upper=clip, inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cabeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the PMI information precomputed based on the PILE co-occurrence counts\n",
    "GENDER_PMI = pd.read_csv(f\"{BASE_DIR}/word2gender_pmi_PILE.csv\", index_col=0)\n",
    "print(len(GENDER_PMI))\n",
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PMI diff used in the main paper\n",
    "PMI_DIFF = get_pmi_diff(GENDER_PMI, \"she\", \"he\").sort_values(\"pmi(she)-pmi(he)\")\n",
    "# rename pmi difference column to be something less verbose :b\n",
    "PMI_DIFF = PMI_DIFF.rename({\"pmi(she)-pmi(he)\": \"pmi_diff\"}, axis=1)\n",
    "PMI_DIFF.sample(15, random_state=81273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec80d3",
   "metadata": {},
   "source": [
    "### Why should we use *PMI_DIFF(w, \"she\", \"he\")*?\n",
    "\n",
    "Why use the pair (\"she\", \"he\") to quantify the association between different words and different genders?\n",
    "Why not use other pair, like (\"mother\", \"father\") or (\"girl\", \"boy\")? \n",
    "\n",
    "In short, we decided to use a pair with good word coverage and that could be used in the most generic contexts.\n",
    "In particular, it appears to be the case that the pairs (\"she\", \"he\") correlate with one another pretty well. \n",
    "Together, these two pairs account for more than 80% of the words.\n",
    "\n",
    "In this section, we compute the Kendall Tau (coefficient B) correlation between the different measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf41275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_pairs_matrix(gender_pmi_df: pd.DataFrame, parallel_terms: list, **kwargs):\n",
    "    # dataframe with all the group pairs PMI (per word)\n",
    "    # (words for which no PMI diff is define)\n",
    "    pairs = gender_pmi_df[[\"word\"]].copy().set_index(\"word\")\n",
    "    num_words = []\n",
    "\n",
    "    for fword, mword in parallel_terms:\n",
    "        try:\n",
    "            # Compute the pmi difference between fword and mword\n",
    "            d = get_pmi_diff(gender_pmi_df, fword, mword, **kwargs).set_index(\"word\")\n",
    "            # Rename to be easier to visualize\n",
    "            d = d.rename({f\"pmi({fword})-pmi({mword})\": f\"{fword}-{mword}\"}, axis=1)\n",
    "            # Number of well-defined words for each of the gender pairs\n",
    "            num_words.append((f\"{fword}-{mword}\", len(d)))\n",
    "            pairs = pairs.join(d[[f\"{fword}-{mword}\"]])\n",
    "        except:\n",
    "            print(f\"Pair ({fword}, {mword}) doesn't exist...\")\n",
    "\n",
    "    return pairs, num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20016c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs_subset, gender_pairs_subset_numwords = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST)\n",
    "correlation_plot(gender_pairs_subset, vmin=-1, vmax=1, square=True, linewidths=.25)#, cbar_kws={\"shrink\": .25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs_all, gender_pairs_subset_clip_numwords = get_gender_pairs_matrix(GENDER_PMI, GROUP_PAIRED_WORDLIST, clip=5)# , missing_val=-50)\n",
    "correlation_plot(gender_pairs_all, vmin=-1, vmax=1, square=True, linewidths=.25)#, cbar_kws={\"shrink\": .25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de062b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDER_PMI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GENDER_PMI[\"pmi__she\"].dropna()) / len(GENDER_PMI[\"pmi__she\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06807311",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GENDER_PMI[\"pmi__he\"].dropna()) / len(GENDER_PMI[\"pmi__he\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.read_csv(\"./pmi_file_w_counts.csv\", index_col=0)\n",
    "freq_she = counts.loc[counts[\"word\"]==\"she\", \"freq\"].item()\n",
    "freq_he = counts.loc[counts[\"word\"]==\"he\", \"freq\"].item()\n",
    "np.log(freq_she), np.log(freq_he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histplot(pmi_df, col, bins, counts=None, binrange=None, **kwargs):\n",
    "    upperbound = None\n",
    "    word = col.split(\"__\")[-1]\n",
    "    if counts is not None:\n",
    "        upperbound = np.log(counts.loc[counts[\"word\"]==word, \"freq\"].item())\n",
    "        if binrange and upperbound not in binrange:\n",
    "            print(upperbound)\n",
    "            upperbound = None\n",
    "    \n",
    "    ax = sns.histplot(data=pmi_df, x=col, bins=bins, kde=False, label=word, binrange=binrange, **kwargs)\n",
    "    \n",
    "    if upperbound is not None:\n",
    "        plt.axvline(upperbound, ls=\"--\", **kwargs)\n",
    "    \n",
    "    # Calculate bin midpoints\n",
    "    counts, bin_edges = np.histogram(pmi_df[col].dropna(), bins=bins, range=binrange)\n",
    "    bin_midpoints = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "\n",
    "    # Create a line connecting the midpoints\n",
    "    sns.scatterplot(x=bin_midpoints, y=counts, **kwargs, s=15)\n",
    "    sns.lineplot(x=bin_midpoints, y=counts, **kwargs, lw=1)\n",
    "    \n",
    "    \n",
    "palette = sns.color_palette(\"colorblind\", n_colors=2)\n",
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__she\", 30, counts, color=palette[0], binrange=(-32, -20))\n",
    "get_histplot(GENDER_PMI, \"pmi__he\", 30, counts, color=palette[1], binrange=(-32, -20))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee354a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3), dpi=150)\n",
    "sns.scatterplot(GENDER_PMI, x=\"pmi__she\", y=\"pmi__he\", s=5)\n",
    "sns.lineplot(GENDER_PMI, x=\"pmi__he\", y=\"pmi__he\", ls=\"--\", color=\"black\", label=\"identity\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, 'she')$\")\n",
    "plt.ylabel(\"$\\mathrm{PMI}(w, 'he')$\")\n",
    "plt.xlim(-34, -20)\n",
    "plt.ylim(-34, -20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(counts) / len(GENDER_PMI)), len(GENDER_PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "sns.regplot(GENDER_PMI, x=\"pmi__he\", y=\"pmi__she\", scatter_kws={\"s\": 1, \"alpha\": 0.2, \"color\": \"gray\", \"edgecolors\": None})\n",
    "sns.lineplot(GENDER_PMI, x=\"pmi__he\", y=\"pmi__he\", ls=\"--\", color=\"black\", label=\"identity\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, 'he')$\")\n",
    "plt.ylabel(\"$\\mathrm{PMI}(w, 'she')$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9929a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=150)\n",
    "sns.jointplot(GENDER_PMI, x=\"pmi__he\", y=\"pmi__she\", s=5, edgecolors=None)#, scatter_kws={\"s\": 1, \"alpha\": 0.2, \"color\": \"gray\", \"edgecolors\": None})\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, 'he')$\")\n",
    "plt.ylabel(\"$\\mathrm{PMI}(w, 'she')$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 3), dpi=150)\n",
    "sns.kdeplot(GENDER_PMI.sample(1000), x=\"pmi__she\", y=\"pmi__he\", fill=True)\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, 'she')$\")\n",
    "plt.ylabel(\"$\\mathrm{PMI}(w, 'he')$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"colorblind\", n_colors=3)\n",
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(gender_pairs_subset, \"she-he\", 30, color=palette[2])\n",
    "\n",
    "mean_val = gender_pairs_subset[\"she-he\"].mean()\n",
    "plt.axvline(mean_val, ls=\"--\", color=\"black\", lw=1)\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, 'she') - \\mathrm{PMI}(w, 'he')$\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs_subset_counts = counts.set_index(\"word\").join(gender_pairs_subset, how=\"left\").sort_values(\"freq\", ascending=False)\n",
    "gender_pairs_corr = gender_pairs_subset_counts.corr(\"kendall\")\n",
    "gender_pairs_corr_cols = gender_pairs_corr.columns[3:]\n",
    "gender_pairs_corr[gender_pairs_corr.index == \"freq\"][gender_pairs_corr_cols].T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87821b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(gender_pairs_subset_counts, x=\"freq\", y=\"hers-his\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d3363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__her\", 30, counts, color=palette[0], binrange=(-32, -20))\n",
    "get_histplot(GENDER_PMI, \"pmi__his\", 30, counts, color=palette[1], binrange=(-32, -20))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f884c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__her\", 30, counts, color=palette[0], binrange=(-32, -20))\n",
    "get_histplot(GENDER_PMI, \"pmi__him\", 30, counts, color=palette[1], binrange=(-32, -20))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__mother\", 30, counts, color=palette[0], binrange=(-32, -20))\n",
    "get_histplot(GENDER_PMI, \"pmi__father\", 30, counts, color=palette[1], binrange=(-32, -20))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__daughter\", 30, counts, color=palette[0], binrange=(-32, -18))\n",
    "get_histplot(GENDER_PMI, \"pmi__son\", 30, counts, color=palette[1], binrange=(-32, -18))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95574",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histplot(GENDER_PMI, \"pmi__mom\", 30, counts,color=palette[0], binrange=(-32,-19))\n",
    "get_histplot(GENDER_PMI, \"pmi__dad\", 30, counts,color=palette[1], binrange=(-32, -19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea268d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3), dpi=150)\n",
    "get_histplot(GENDER_PMI, \"pmi__wife\", 30, counts, color=palette[0], binrange=(-32, -20))\n",
    "get_histplot(GENDER_PMI, \"pmi__husband\", 30, counts, color=palette[1], binrange=(-32, -20))\n",
    "plt.ylabel(\"Number of words\")\n",
    "plt.xlabel(\"$\\mathrm{PMI}(w, g_i)$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4724c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histplot(GENDER_PMI, \"pmi__girl\", 30, counts,color=palette[0], binrange=(-32,-19))\n",
    "get_histplot(GENDER_PMI, \"pmi__boy\", 30, counts,color=palette[1], binrange=(-32, -19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db5065",
   "metadata": {},
   "source": [
    "## Having settled on a specific gender diff PMI \n",
    "\n",
    "Say, PMI_DIFF(w, she, he), let us now compute the pmi of the words used for each of the benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb3075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally this will be the same base dir as before, once we put this into a repository we can update this\n",
    "BASE_DIR = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86298ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"== Number of Words ==\")\n",
    "# we use the same words for all the templates\n",
    "OUR_WORDS = pd.read_csv(f\"{BASE_DIR}/results-words5/final-results/revised_templates.csv\", index_col=0)\n",
    "# comment the line below to remove the filtering by is_natural (it differs in the words : {'dudgeon', 'forfend'})\n",
    "OUR_WORDS = OUR_WORDS[OUR_WORDS[\"is_natural\"]][\"word\"].unique()\n",
    "print(\"Ours:\", len(OUR_WORDS))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Read Winobias and apply some post processing\n",
    "# --------------------------------------------------\n",
    "WINOBIAS_WORDS = pd.read_csv(f\"{BASE_DIR}/results-baselines/coref__Winobias__templates.dev.csv\", index_col=0)\n",
    "WINOBIAS_WORDS = WINOBIAS_WORDS[\"word\"].apply(lambda x: x.split()[-1].lower()).unique()\n",
    "print(\"WinoBias:\", len(WINOBIAS_WORDS))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Read Winogender and apply some post processing\n",
    "# --------------------------------------------------\n",
    "WINOGENDER_WORDS = pd.read_csv(f\"{BASE_DIR}/results-baselines/coref__Winogender__templates.csv\", index_col=0)\n",
    "WINOGENDER_WORDS = np.unique(\n",
    "    # Sentences are constructed based on two attribute words, so we'll consider both :3\n",
    "    WINOGENDER_WORDS[\"example_id\"].apply(lambda x: x.split(\".\")[0].lower()).unique().tolist()\n",
    "    + WINOGENDER_WORDS[\"example_id\"].apply(lambda x: x.split(\".\")[1].lower()).unique().tolist()\n",
    ")\n",
    "print(\"Winogender:\", len(WINOGENDER_WORDS))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Read Stereoset and apply some post processing\n",
    "# --------------------------------------------------\n",
    "STEREOSET_WORDS = pd.read_csv(f\"{BASE_DIR}/results-baselines/lm__StereoSet_pronouns_only.csv\")[\"word\"].unique()\n",
    "print(\"Stereoset (gender, pronoun subset):\", len(STEREOSET_WORDS))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Read CROW-S Pairs and apply some post processing\n",
    "# --------------------------------------------------\n",
    "# We exclude Crow-S in a first stage because it doesn't really\n",
    "# provide a clear annotation on what are the words. Since the\n",
    "# templates were written by annotators. In the future, we\n",
    "# can try to derive it heuristically using dependency trees\n",
    "# guided by the main changei n the sentence \n",
    "# (e.g., if a pronoun is changed we can try to link that back to the entity it was depending or describing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4791dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(421)\n",
    "sorted(np.random.choice(OUR_WORDS, 50, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904cfce",
   "metadata": {},
   "source": [
    "**Compute the embeddings of every word**: We will use fasttext models to compute a 300-dimensional embedding for every defined word in PMI_DIFF. We opt for char-level embedding as opposed to word-level embedding because we're dealing with words altered by suffixes and prefixes and for which there's sometimes not appropriate word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01faaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util, functools\n",
    "# fasttext.util.download_model('en', if_exists='ignore')\n",
    "# !mv cc.en.300.bin /extra/ucinlp1/cbelem/fasttext/cc.en.300.bin\n",
    "# character 5-grams\n",
    "ft = fasttext.load_model('/extra/ucinlp1/cbelem/fasttext/cc.en.300.bin')\n",
    "\n",
    "\n",
    "def word_top1_nearest_neighbor(word: str, word2emb: dict) -> str:\n",
    "    \"\"\"Compute the specified word's nearest neighbor using the\n",
    "    word-to-embedding mapping. This word-embedding mapping should\n",
    "    be precomputed \n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    if word2emb.get(word) is not None:\n",
    "        return word\n",
    "    \n",
    "    cand_words, cand_embs = zip(*word2emb.items())\n",
    "    cand_embs = np.vstack(cand_embs)\n",
    "    \n",
    "    word_emb = ft.get_word_vector(word).reshape(1, -1)\n",
    "    similarities = cosine_similarity(word_emb, cand_embs)\n",
    "    return cand_words[np.argmax(similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce177f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 pmi mapping\n",
    "def collect_word_level_pmi(wordlist, pmi_diff: pd.DataFrame, nn_fn: callable, pmi_colname=\"pmi_diff\"):\n",
    "    word2pmi = pmi_diff[[\"word\", pmi_colname]].set_index(\"word\").to_dict()[pmi_colname]\n",
    "    \n",
    "    results = []\n",
    "    approx_pairs = []\n",
    "    results_w_approx = []\n",
    "    for word in wordlist:\n",
    "        \n",
    "        if (pmi_val := word2pmi.get(word)) is not None:\n",
    "            results.append(pmi_val)\n",
    "            results_w_approx.append(pmi_val)\n",
    "        else:\n",
    "            cand_word = nn_fn(word)\n",
    "            pmi_val = word2pmi.get(cand_word)\n",
    "            \n",
    "            results_w_approx.append(pmi_val)\n",
    "            approx_pairs.append({\"word\": word, \"approx\": cand_word})\n",
    "            \n",
    "    return results, results_w_approx, approx_pairs\n",
    "\n",
    "\n",
    "# If you plan to collect word level pmi based on PMI_DIFF\n",
    "_PMI_REF = PMI_DIFF\n",
    "\n",
    "# Compute the 300-dimensional word embedding representation for all words in PMI_DIFF\n",
    "WORDS2EMB_FT = {w: ft.get_word_vector(w.strip()) for w in _PMI_REF.word.values.tolist()}\n",
    "fasttext_nearest_neighbor = functools.partial(word_top1_nearest_neighbor, word2emb=WORDS2EMB_FT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATANAME_TO_WORDLIST = {\n",
    "    \"Ours\": OUR_WORDS,\n",
    "    \"Winobias\": WINOBIAS_WORDS,\n",
    "    \"Winogender\": WINOGENDER_WORDS,\n",
    "    \"Stereoset\": STEREOSET_WORDS,\n",
    "}\n",
    "\n",
    "# All the dataset names we're using in our analysis\n",
    "DATANAMES = DATANAME_TO_WORDLIST.keys()\n",
    "\n",
    "RESULTS = {\"pmi\": defaultdict(list), \"len\": {}, \"missing\": {}}\n",
    "for name, wordlist in DATANAME_TO_WORDLIST.items():\n",
    "    results, _, missing_words = collect_word_level_pmi(wordlist, pmi_diff=_PMI_REF, pmi_colname=\"pmi_diff\", nn_fn=fasttext_nearest_neighbor)\n",
    "    \n",
    "    RESULTS[\"pmi\"][\"dataset\"].extend([name] *len(results)) \n",
    "    RESULTS[\"pmi\"][\"pmi_val\"].extend(results)\n",
    "    \n",
    "    RESULTS[\"len\"][name] = len(results)\n",
    "    RESULTS[\"missing\"][name] = missing_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f5110",
   "metadata": {},
   "source": [
    "### Hypothesis 1: Our words correlate less strongly with gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694079eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3), dpi=(200))\n",
    "sns.boxplot(pd.DataFrame(RESULTS[\"pmi\"]), x=\"dataset\", y=\"pmi_val\")\n",
    "plt.xlabel(\"Dataset name\")\n",
    "plt.ylabel(\"PMI difference\")\n",
    "plt.ylim(-2, 2.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d139da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(RESULTS[\"pmi\"]).groupby(\"dataset\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2eda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.DataFrame(RESULTS[\"pmi\"])\n",
    "_df[\"pmi_val_abs\"] = _df[\"pmi_val\"].apply(np.abs)\n",
    "_df.sort_values(\"pmi_val_abs\", ascending=False).groupby(\"dataset\").head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.sort_values(\"pmi_val_abs\", ascending=False).groupby(\"dataset\").max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fa705",
   "metadata": {},
   "source": [
    "### Hypothesis 2: Our words are more diverse than other studies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee94892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "diversity_results = defaultdict(list)\n",
    "for name1, name2 in itertools.product(DATANAMES, DATANAMES):\n",
    "    \n",
    "    if name1 == name2:\n",
    "        diversity_results[\"jaccard_similarity\"].append(1)\n",
    "    else:\n",
    "        words1 = Counter(DATANAME_TO_WORDLIST[name1])\n",
    "        words2 = Counter(DATANAME_TO_WORDLIST[name2])\n",
    "        diversity_results[\"jaccard_similarity\"].append(len(words1 & words2) / len(words1 | words2))\n",
    "        \n",
    "    diversity_results[\"dataset1\"].append(name1)\n",
    "    diversity_results[\"dataset2\"].append(name2)\n",
    "    \n",
    "diversity_results = pd.DataFrame(diversity_results)\n",
    "jacc_sim_table = diversity_results.pivot(index=\"dataset1\", columns=\"dataset2\", values=[\"jaccard_similarity\"])\n",
    "jacc_sim_table.to_csv(\"./tables/word_level__jaccardsimilarity.csv\", index=None)\n",
    "\n",
    "# Note that while the stereoset does not have any jaccard similarity with the other datasets\n",
    "# we found that there is high similarity and also there are some gendered expressions.\n",
    "print(jacc_sim_table.droplevel(level=None, axis=1).to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    caption=\"Attribute lexicon overlap (as determined by the Jaccard Similarity coefficient).\",\n",
    "    label=\"tab:word-level-jacc-similarity-no-stem\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e911a",
   "metadata": {},
   "source": [
    "It may be more meaningful to remove the plurals of the words and ensure everything is in the same format. Maybe more aggressively, we can also apply the lancaster stemmer (since it also reduces words like anorexia and anorexic to the same root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd478fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "DATANAME_TO_STEMLIST = {name: [snow_stemmer.stem(w) for w in wordlist] for name, wordlist in DATANAME_TO_WORDLIST.items()}\n",
    "\n",
    "diversity_results = defaultdict(list)\n",
    "for name1, name2 in itertools.product(DATANAMES, DATANAMES):\n",
    "    \n",
    "    if name1 == name2:\n",
    "        diversity_results[\"jaccard_similarity\"].append(1)\n",
    "    else:\n",
    "        words1 = Counter(DATANAME_TO_STEMLIST[name1])\n",
    "        words2 = Counter(DATANAME_TO_STEMLIST[name2])\n",
    "        diversity_results[\"jaccard_similarity\"].append(len(words1 & words2) / len(words1 | words2))\n",
    "        \n",
    "    diversity_results[\"dataset1\"].append(name1)\n",
    "    diversity_results[\"dataset2\"].append(name2)\n",
    "    \n",
    "diversity_results = pd.DataFrame(diversity_results)\n",
    "jacc_sim_table = diversity_results.pivot(index=\"dataset1\", columns=\"dataset2\", values=[\"jaccard_similarity\"])\n",
    "jacc_sim_table.to_csv(\"./tables/word_level__snowballstem_jaccardsimilarity.csv\", index=None)\n",
    "\n",
    "# Note that while the stereoset does not have any jaccard similarity with the other datasets\n",
    "# we found that there is high similarity and also there are some gendered expressions.\n",
    "print(jacc_sim_table.droplevel(level=None, axis=1).to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    caption=\"Attribute lexicon overlap (as determined by the Jaccard Similarity coefficient). The lexicon was normalized by applying Snowball Stemmer.\",\n",
    "    label=\"tab:word-level-jacc-similarity-snowball-stem\",\n",
    "))\n",
    "jacc_sim_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3), dpi=(200))\n",
    "sns.barplot(pd.DataFrame(RESULTS[\"len\"].items(), columns=[\"dataset\", \"num_words\"]), y=\"dataset\", x=\"num_words\")\n",
    "plt.xlabel(\"Number of attribute words\")\n",
    "plt.ylabel(\"Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./images/word_level__barplot_num_words_per_dataset.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15820fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "d = pd.DataFrame(RESULTS[\"pmi\"])\n",
    "d = d.sort_values([\"dataset\", \"pmi_val\"])\n",
    "d[\"pmi_val_abs\"] = d[\"pmi_val\"].apply(np.abs)\n",
    "\n",
    "# Plot the increase in the epsilon (from 0 to 2.5) and the impact in the number of words included\n",
    "epsilons = np.linspace(0, 2.1, 101)\n",
    "\n",
    "diversity_results = defaultdict(list)\n",
    "for dataset, eps in itertools.product(DATANAME_TO_WORDLIST.keys(), epsilons):\n",
    "    counts = ((d[\"pmi_val_abs\"] <= eps) & (d[\"dataset\"] == dataset)).sum() \n",
    "    diversity_results[\"dataset\"].append(dataset)\n",
    "    diversity_results[\"num_words\"].append(counts)\n",
    "    diversity_results[\"epsilon\"].append(eps)\n",
    "    \n",
    "diversity_results = pd.DataFrame(diversity_results)\n",
    "sns.lineplot(diversity_results, x=\"epsilon\", y=\"num_words\", hue=\"dataset\")\n",
    "plt.xlabel(\"Gender Correlation Strength ($\\epsilon_P$)\")\n",
    "plt.ylabel(\"Number of attribute words\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./images/word_level__num_words_by_gendercorrelation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(RESULTS[\"pmi\"])\n",
    "d = d.sort_values([\"dataset\", \"pmi_val\"])\n",
    "d[\"pmi_val_abs\"] = d[\"pmi_val\"].apply(np.abs)\n",
    "\n",
    "diversity_results = defaultdict(list)\n",
    "for dataset, eps in itertools.product(DATANAME_TO_WORDLIST.keys(), epsilons):\n",
    "    freq = ((d[\"pmi_val_abs\"] <= eps) & (d[\"dataset\"] == dataset)).sum() / len(DATANAME_TO_WORDLIST[dataset])\n",
    "    diversity_results[\"dataset\"].append(dataset)\n",
    "    diversity_results[\"num_words\"].append(freq)\n",
    "    diversity_results[\"epsilon\"].append(eps)\n",
    "    \n",
    "diversity_results = pd.DataFrame(diversity_results)\n",
    "sns.lineplot(diversity_results, x=\"epsilon\", y=\"num_words\", hue=\"dataset\")\n",
    "plt.xlabel(\"Correlation Strength with Gender ($\\epsilon_P$)\")\n",
    "plt.ylabel(\"% of attribute words\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./images/word_level__freq_words_by_gendercorrelation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01a4d3",
   "metadata": {},
   "source": [
    "### Hypothesis 3: The selected words are not that similar to the ones in other studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATANAME_TO_WORD_EMB = {\n",
    "    name: np.vstack([WORDS2EMB_FT[w] for w in wordlist if WORDS2EMB_FT.get(w) is not None])\n",
    "    for name, wordlist in DATANAME_TO_WORDLIST.items()\n",
    "}\n",
    "\n",
    "# All the dataset names we're using in our analysis\n",
    "DATANAMES = DATANAME_TO_WORD_EMB.keys()\n",
    "DATANAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa1ccc",
   "metadata": {},
   "source": [
    "#### Average distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cdcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "semantics_corr = defaultdict(list)\n",
    "for name1, name2 in itertools.product(DATANAMES,DATANAMES):\n",
    "    if name1 == name2:\n",
    "        semantics_corr[\"similarity\"].append(1)\n",
    "    else:\n",
    "        sim = cosine_similarity(DATANAME_TO_WORD_EMB[name1], DATANAME_TO_WORD_EMB[name2])\n",
    "        # sim will be a matrix of size num_words_in_name1 x num_words_in_name2\n",
    "        \n",
    "        # let's compute the average distance of the \n",
    "        semantics_corr[\"similarity\"].append(np.abs(sim).mean(axis=1).mean())\n",
    "\n",
    "    semantics_corr[\"dataset1\"].append(name1)\n",
    "    semantics_corr[\"dataset2\"].append(name2)\n",
    "    \n",
    "semantics_corr = pd.DataFrame(semantics_corr)\n",
    "semantics_corr.pivot(index=\"dataset1\", columns=\"dataset2\", values=[\"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "semantics_corr = defaultdict(list)\n",
    "\n",
    "for name1, name2 in itertools.product(DATANAMES,DATANAMES):\n",
    "    if name1 == name2:\n",
    "        semantics_corr[\"similarity\"].append(1)\n",
    "    else:\n",
    "        sim = cosine_similarity(DATANAME_TO_WORD_EMB[name1], DATANAME_TO_WORD_EMB[name2])\n",
    "        # sim will be a matrix of size num_words_in_name1 x num_words_in_name2\n",
    "        # let's compute the average distance of the \n",
    "        semantics_corr[\"similarity\"].append(np.median(np.abs(sim).max(axis=1)))\n",
    "\n",
    "    semantics_corr[\"dataset1\"].append(name1)\n",
    "    semantics_corr[\"dataset2\"].append(name2)\n",
    "    \n",
    "semantics_corr = pd.DataFrame(semantics_corr)\n",
    "corr_table = semantics_corr.pivot(index=\"dataset1\", columns=\"dataset2\", values=[\"similarity\"])\n",
    "corr_table.to_csv(\"./tables/word_level__avg_max_fasttext_wordtoword_cosine_similarity.csv\", index=None)\n",
    "print(corr_table.droplevel(level=None, axis=1).to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    caption=\"Average distribution of the cosine similarity with the nearest neighboring word (as determined by \\\\texttt{fasttext}).\",\n",
    "    label=\"tab:word-level-avg-nearestneighbor-cosine-sim\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# - get a few qualitative examples of what are the most similar words.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad063317",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_words_results = defaultdict(list)\n",
    "\n",
    "gendered_words = {}\n",
    "for name, wordlist in DATANAME_TO_WORDLIST.items():   \n",
    "    gendered_words[name] = {\n",
    "        \"female\": Counter(FEMALE_WORDS) & Counter(wordlist),\n",
    "        \"male\": Counter(MALE_WORDS) & Counter(wordlist),\n",
    "    }\n",
    "    \n",
    "    gendered_words_results[\"counts\"].append(len(gendered_words[name][\"female\"]))\n",
    "    gendered_words_results[\"counts\"].append(len(gendered_words[name][\"male\"]))\n",
    "    gendered_words_results[\"gender\"].extend([\"female\", \"male\"])\n",
    "    gendered_words_results[\"dataset\"].extend([name, name])\n",
    "\n",
    "\n",
    "gendered_words_results = pd.DataFrame(gendered_words_results)\n",
    "gendered_words_results.groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 5\n",
    "\n",
    "# Necessary bcause some words are not defined and if we use indices then\n",
    "# we wont be able to map it back\n",
    "_DATANAME_TO_WORDLIST = {\n",
    "    name: np.vstack([w for w in wordlist if WORDS2EMB_FT.get(w) is not None])\n",
    "    for name, wordlist in DATANAME_TO_WORDLIST.items()\n",
    "}\n",
    "\n",
    "\n",
    "semantics_corr = defaultdict(list)\n",
    "semantics_words = defaultdict(list)\n",
    "\n",
    "for name1, name2 in itertools.product(DATANAMES,DATANAMES):\n",
    "    if name1 == name2:\n",
    "        semantics_corr[\"similarity\"].append(1)\n",
    "    else:\n",
    "        sim = cosine_similarity(DATANAME_TO_WORD_EMB[name1], DATANAME_TO_WORD_EMB[name2])\n",
    "        # sim will be a matrix of size num_words_in_name1 x num_words_in_name2\n",
    "        sim_values = np.abs(sim)\n",
    "        top_idx = np.argsort(sim_values, axis=1)[:,-TOP:]\n",
    "        for i, row in enumerate(top_idx):\n",
    "            top_related = (np.array(_DATANAME_TO_WORDLIST[name2])[row]).tolist()\n",
    "            top_related = [(w, sim[i, j]) for w, j in zip(top_related, row)]\n",
    "            \n",
    "            semantics_words[f\"{name1}__{name2}\"].append((_DATANAME_TO_WORDLIST[name1][i][0], top_related))\n",
    "        # let's compute the average distance of the \n",
    "        semantics_corr[\"similarity\"].append(f\"{np.mean(sim_values.max(axis=1)):.2f} (Â± {np.std(sim_values.max(axis=1)):.2f})\")\n",
    "\n",
    "    semantics_corr[\"dataset1\"].append(name1)\n",
    "    semantics_corr[\"dataset2\"].append(name2)\n",
    "    \n",
    "semantics_corr = pd.DataFrame(semantics_corr)\n",
    "corr_table = semantics_corr.pivot(index=\"dataset1\", columns=\"dataset2\", values=[\"similarity\"])\n",
    "corr_table.to_csv(\"./tables/word_level__avg_max_fasttext_wordtoword_cosine_similarity.csv\", index=None)\n",
    "print(corr_table.droplevel(level=None, axis=1).to_latex(\n",
    "    float_format=\"%.2f\",\n",
    "    caption=\"Average distribution of the cosine similarity with the nearest neighboring word (as determined by \\\\texttt{fasttext}).\",\n",
    "    label=\"tab:word-level-avg-nearestneighbor-cosine-sim\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"./others/top-{TOP}-similarwords.pkl\", \"wb\") as f:\n",
    "    pickle.dump(semantics_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96088650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ea84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
